{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dd05d8f",
   "metadata": {},
   "source": [
    "---\n",
    "title:  \"[Kaggle] BYU - Locating Bacterial Flagellar Motors 2025\"\n",
    "header:\n",
    "   teaser: \"/assets/images/BYU images/header.png\" \n",
    "excerpt: \"캐글 대회에서 실패를 통해 배운 교훈\"\n",
    "categories: \n",
    "- AI\n",
    "- Detection\n",
    "tags:\n",
    "- competition\n",
    "toc_label: Contents\n",
    "toc: true\n",
    "toc_sticky: True\n",
    "toc_h_min: 1\n",
    "toc_h_max: 3\n",
    "date: 2025-09-04\n",
    "last_modified_at: 2025-09-04\n",
    "---\n",
    "\n",
    "# About Competition\n",
    "## Overview\n",
    "\n",
    "![image](/assets/images/BYU images/header.png){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "[BYU - Locating Bacterial Flagellar Motors 2025](https://www.kaggle.com/competitions/byu-locating-bacterial-flagellar-motors-2025)\n",
    "\n",
    "\n",
    "**대회 기간**: 2025.03.06 ~ 2025.06.04\n",
    "\n",
    "**문제 및 목표**: 3차원의 박테리아를 단층 촬영한 이미지들(tomogram)이 있을 때, 박테리아의 모터 좌표(x,y,z)를 탐지한다.\n",
    "\n",
    "**박테리아의 모터란?**: 박테리아는 이동하기 위한 편모(꼬리처럼 생김)가 있고 편모와 몸체가 연결된 부분이 모터다. 박테리아는 지구상에서 유일하게 원운동(바퀴처럼 완전히 회전)하는 기관(모터)을 가진 유기물이다.\n",
    "\n",
    "![image](/assets/images/BYU images/tomogram.gif){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "위 영상은 단층 촬영한 이미지들을 쌓아서 영상으로 만든 것 입니다. 짧게 빨간 점이 점멸하는 부분이 라벨링된 모터의 좌표(x,y,z)입니다.\n",
    "\n",
    "***\n",
    "\n",
    "이번 캐글 대회에서 수상하지는 못했지만 과정을 통해 배운점이 많습니다. 실패를 통해 배운 교훈을 설명하겠습니다.\n",
    "\n",
    "## Dataset\n",
    "![image](/assets/images/BYU images/Dataset.png){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "학습 데이터, 검증 데이터 각각 단층 촬영된 박테리아의 폴더(e.g. tomo_00e047)가 있고 각 폴더 안에는 단층 촬영된 이미지(e.g. slice_0000.jpg)들이 있습니다.\\\n",
    "학습 데이터에는 tomogram이 737개 존재하며, 한 tomogram 당 300~600개의 이미지(slice)가 존재합니다.\n",
    "\n",
    "### train_labels.csv\n",
    "![image](/assets/images/BYU images/train_labels.png){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "**train_labels.csv의 컬럼 요약** : `tomogram의 고유  id` , `z, y, x 좌표` , `이미지의 z`, `height, width 크기` , `이미지의 한 픽셀 당 담긴angstroms(원자 하나의 지름과 비슷한 거리 단위) 크기` , `한 tomogram에 있는 모터의 개수`\n",
    "\n",
    "test 데이터에는 더미로 학습 데이터에 있는 tomogram 데이터가 3개 들어있고, 제출했을 때 공개되지 않은 900개의 tomogram으로 점수가 평가됩니다. **실제 test 데이터에는 모터가 없거나 모터가 딱 하나인 데이터들만 존재**합니다.\n",
    "\n",
    "### Evaluation\n",
    "![image](/assets/images/BYU images/metric.png){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "- 예측한 모터의 좌표가 정답 좌표와 1000 angstroms 내에 있으면 TP, 밖에 있으면 FN으로 개수를 셉니다.\n",
    "- $F_\\beta$-score로 평가합니다. \n",
    "\n",
    "# Workflow\n",
    "## 1. 빠른 제출 \n",
    "![image](/assets/images/BYU images/baseline code.png){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "다른 참여자가 작성한 **baseline** 코드를 복사해서 **데이터 생성, 모델 학습, 제출** 과정까지 빠르게 거쳤습니다. 그랬을 때 **public 점수**는 **0.507**이었습니다. 당시 **1등**의 점수는 **0.88**였습니다.\n",
    "\n",
    "해당 코드에서 이미지가 가지고 있는 값의 상위 2%, 하위 2%를 제거하고 제거한만큼 중간 값의 범위를 늘리는 방식을 사용했습니다. 이를 통해 흐릿한 이미지의 대비를 더 극명하게 할 수 있음으로 저도 제 코드에 적극 활용했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline에 있는 코드\n",
    "def normalize_slice(slice_data):\n",
    "    \"\"\"\n",
    "    Normalize slice data using 2nd and 98th percentiles\n",
    "    \"\"\"\n",
    "    # Calculate percentiles\n",
    "    p2 = np.percentile(slice_data, 2)\n",
    "    p98 = np.percentile(slice_data, 98)\n",
    "    \n",
    "    # Clip the data to the percentile range\n",
    "    clipped_data = np.clip(slice_data, p2, p98)\n",
    "    \n",
    "    # Normalize to [0, 255] range\n",
    "    normalized = 255 * (clipped_data - p2) / (p98 - p2)\n",
    "    \n",
    "    return np.uint8(normalized)\n",
    "\n",
    "# 내가 수정한 torch 버전\n",
    "def contrast_extension(self, data):\n",
    "    p2 = torch.quantile(data, 0.02)\n",
    "    p98 = torch.quantile(data, 0.98)\n",
    "    \n",
    "    # 클리핑\n",
    "    clipped_data = torch.clamp(data, min=p2, max=p98)\n",
    "    \n",
    "    # 0~1 범위로 규제\n",
    "    return (clipped_data - p2) / (p98 - p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f0282",
   "metadata": {},
   "source": [
    "## 2. 모델선정\n",
    "\n",
    "일반적으로 decetion에서 사용하는 모델이 yolo임으로 저도 **yolo를 채택**했습니다./\n",
    "모델의 아키텍처를 개조하고 싶어 처음에는 구조가 간단한 **yolo8**를 사용하였고, 나중에는 **yolo11**도 사용해 성능을 확인했습니다. \n",
    "\n",
    "## 3. Overlay로 데이터 생성\n",
    "\n",
    "yolo는 태생적으로 **채널이 3(RGB)이거나 1(gray)인 이미지만**을 입력으로 받을 수 있기 때문에 3차원 정보를 담은 tomogram을 어떻게 yolo의 입력으로 넣을 수 있을지 고민하였습니다. 첫 번째로 선택한 방법이 **오버레이(overlay)로 3차원 이미지들을 하나로 합쳐 2차원에 모두 담길 수 있도록 하는 것**이었습니다.\\\n",
    "모터의 위치를 추정하기 위해서는 모델이 **편모의 전체적인 형태**를 파악하는 것이 유리할 것입니다. 그런데 개별적인 2차원 이미지를 봤을 때 편모는 일부만 드러나고 끊겨서 보입니다. 그렇기에 3차원의 이미지들을 모델이 한 번에 볼 수 있도록 하는 것이 좋다고 생각했습니다.\\\n",
    "하지만 오버레이 방식은 결과적으로 **한계**가 있다고 판단하여 사용하지 않았습니다. 그렇게 판단하게된 과정을 설명하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_blend(background, foreground):\n",
    "    \"\"\"Overlay 블렌딩 모드 적용\"\"\"\n",
    "    result = torch.where(\n",
    "        background < 0.5,\n",
    "        2 * background * foreground,  # Multiply 적용\n",
    "        1 - (2 * (1 - background) * (1 - foreground)) # -0.1* background**2 # Screen 적용\n",
    "    )\n",
    "    return torch.clip(result, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaef924",
   "metadata": {},
   "source": [
    "오버레이는 배경 이미지의 픽셀 값이 어두우면 더 어두워지도록 얹는 이미지의 픽셀 값과 **Multiply**하고, 밝으면 더 밝아지도록 얹는 이미지의 픽셀 값과 **Screen** 연산을 합니다.\n",
    "\n",
    "하지만 오버레이에는 몇 가지 **문제**가 있었습니다. \n",
    "\n",
    "### 오버레이 첫 번째 문제\n",
    "![image](/assets/images/BYU images/overlay1.png){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "**적은 수의 이미지**가 오버레이 됐을 때는 **괜찮은 퀄리티**로 이미지가 **합쳐**지지만 **많이 쌓을 수록** 편모의 형태가 **소실**됩니다. 이를 해결하기 위해 오랜 시간을 들여 **수학식**을 개선해보려 했지만, 새로운 이미지를 쌓을수록 얹는 이미지의 **밝은 색(값)도 계속 쌓임**으로 Multiply를 한다고 해도 편모의 픽셀 값이 **결국엔 밝아져 형태가 희미**해지는 문제가 생깁니다.\\\n",
    "이를 수학적으로 해결하는 것에는 **한계**가 있다고 생각하여 **오버레이할 이미지만을 판별**할 수 있는 **binary classification** 모델을 만들기로 했습니다. \n",
    "오버레이할 이미지는 **박테리아의 존재 여부**로 판별됩니다. 모터(그리고 편모)는 없지만 박테리아의 몸통만 존재하는 tomogram이 있음으로 모터가 없는 tomogram과 있는 tomogram으로 분류 모델을 학습시키는 것이 아니라, 모터가 있는 tomogram의 **정답 z**에서 **근접 범위를 박테리아가 있는 이미지**, **멀리 떨어진 이미지를 박테리아가 없는 이미지**로 학습 데이터를 구성해서 모델을 학습시켰습니다. \n",
    "\n",
    "아래는 **binary classification** 모델을 학습시키는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision.models as models\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torchvision \n",
    "import torch.optim as optim \n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils import data \n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision.models import ResNet34_Weights\n",
    "from collections import Counter\n",
    "import polars as pl\n",
    "import os\n",
    "import math as m\n",
    "from tqdm.auto import tqdm \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_rate = 0.9\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccec981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir = './', split='train', validation=False, train_val_rate=0.9, base_transform=None, aug_transform=None): \n",
    "        super(Classification_Dataset, self).__init__() # 상속\n",
    "        \n",
    "        self.df = pl.read_csv(\"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/train_labels.csv\", columns=[\"tomo_id\",\"Motor axis 0\",\"Voxel spacing\"], low_memory=True)\n",
    "        limit_angstroms = 1000 / 3**(1/2) # 박테리아가 있다고 예측할 데이터 경계(angstroms)\n",
    "        outlier_angstroms = 2000 # 박테리아가 없다고 예측할 데이터 경계(angstroms)\n",
    "        \n",
    "        self.top_directory = os.path.join(root_dir, split) \n",
    "\n",
    "        self.data_list = []  # self.data_list에 모든 파일 목록 저장\n",
    "        for sub_directory in os.listdir(self.top_directory):\n",
    "            full_directory = os.path.join(self.top_directory, sub_directory)\n",
    "\n",
    "            df_current_tomo = self.df.filter(pl.col(\"tomo_id\")==sub_directory)\n",
    "            voxel_spacing = df_current_tomo.select(\"Voxel spacing\")[0].item() \n",
    "            limit_z = m.trunc(limit_angstroms / voxel_spacing)\n",
    "            outlier_z = m.trunc(outlier_angstroms / voxel_spacing) \n",
    "            gt_max = int(df_current_tomo.select(\"Motor axis 0\").max().item())\n",
    "            gt_min = int(df_current_tomo.select(\"Motor axis 0\").min().item())\n",
    "            \n",
    "            if gt_max != -1: # 모터가 있는 데이터로만 학습 데이터를 구성\n",
    "                for file_name in os.listdir(full_directory):\n",
    "                    # int(file_name[6:-4] : 현재의 z값\n",
    "                    if (gt_min - limit_z) <= int(file_name[6:-4]) and (gt_max + limit_z) >= int(file_name[6:-4]): # 박테리아가 있는 z일 경우\n",
    "                        self.data_list.append(os.path.join(full_directory, file_name)+\"1\") # 라벨을 1로 지정\n",
    "                    if (gt_min - outlier_z) >= int(file_name[6:-4]) or (gt_max + outlier_z) <= int(file_name[6:-4]): # 박테리아가 없는 z일 경우\n",
    "                        self.data_list.append(os.path.join(full_directory, file_name)+\"0\") # 라벨을 0로 지정\n",
    "\n",
    "        self.validation = validation\n",
    "        self.split = split\n",
    "        self.train_rate = train_val_rate\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            train_len = int(round(len(self.data_list) * self.train_rate))\n",
    "            if self.validation == False:\n",
    "                self.data_list = self.data_list[:train_len]\n",
    "                \n",
    "            if self.validation == True:\n",
    "                self.data_list = self.data_list[train_len:]\n",
    "        \n",
    "        self.base_transform = base_transform\n",
    "        self.aug_transform = aug_transform\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        image_path = self.data_list[index][:-1] \n",
    "        image = Image.open(image_path)\n",
    "        label = int(self.data_list[index][-1])\n",
    "        \n",
    "        if (self.split =='train') and (self.validation==False):\n",
    "            image = self.aug_transform(image)\n",
    "            \n",
    "        if (self.split =='test') or (self.validation==True):\n",
    "            image = self.base_transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    base_transform = transforms.Compose([\n",
    "                    transforms.Resize((256, 256)),\n",
    "                    transforms.ToTensor(), # (28, 28, 1) -> (1, 28, 28) // 0 ~ 255(grayscale) -> 0 ~ 1\n",
    "                    transforms.Normalize(0.5, 0.5) # standard scaling # [0, 1] -> [-0.5, 0.5] -> [-1, 1]\n",
    "                    ])\n",
    "\n",
    "aug_transform = transforms.Compose([\n",
    "                    transforms.Resize((256, 256)),\n",
    "                    transforms.RandomPerspective(distortion_scale=0.2, p=0.2), # 찌그러트리기\n",
    "                    transforms.RandomAffine(degrees=90, translate=(0.1, 0.1), scale=(0.8, 1.2)), # 회전 이동 자동 적용\n",
    "                    transforms.RandomAutocontrast(p=0.2), # 밝고 어두운 부분의 차이를 자동으로 조절\n",
    "                    transforms.RandomHorizontalFlip(p=0.2), # 좌우반전\n",
    "                    transforms.RandomVerticalFlip(p=0.2), # 상하반전\n",
    "                    transforms.ToTensor(), # (28, 28, 1) -> (1, 28, 28) // 0 ~ 255(grayscale) -> 0 ~ 1\n",
    "                    transforms.Normalize(0.5, 0.5) # standard scaling # [0, 1] -> [-0.5, 0.5] -> [-1, 1]\n",
    "                    ])\n",
    "\n",
    "def sampler(validation = False, train_val_rate = 0.9):\n",
    "    df = pl.read_csv(\"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/train_labels.csv\", columns=[\"tomo_id\",\"Motor axis 0\",\"Voxel spacing\"], low_memory=True)\n",
    "    labels = []  \n",
    "    top_directory = '/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/train'\n",
    "    limit_angstroms = 1000 / 3 # 박테리아가 있다고 예측할 데이터 경계(angstroms)\n",
    "    outlier_angstroms = 2000 \n",
    "    for sub_directory in os.listdir(top_directory):\n",
    "        full_directory = os.path.join(top_directory, sub_directory)\n",
    "    \n",
    "        df_current_tomo = df.filter(pl.col(\"tomo_id\")==sub_directory)\n",
    "        voxel_spacing = df_current_tomo.select(\"Voxel spacing\")[0].item() \n",
    "        limit_z = m.trunc(limit_angstroms / voxel_spacing)\n",
    "        outlier_z = m.trunc(outlier_angstroms / voxel_spacing) \n",
    "        gt_max = int(df_current_tomo.select(\"Motor axis 0\").max().item())\n",
    "        gt_min = int(df_current_tomo.select(\"Motor axis 0\").min().item())\n",
    "        \n",
    "        if gt_max != -1:\n",
    "            for file_name in os.listdir(full_directory):\n",
    "                # int(file_name[6:-4] : 현재의 z값\n",
    "                if (gt_min - limit_z) <= int(file_name[6:-4]) and (gt_max + limit_z) >= int(file_name[6:-4]): # 박테리아가 있는 z일 경우\n",
    "                    labels.append(1) # 라벨을 1로 지정\n",
    "                if (gt_min - outlier_z) >= int(file_name[6:-4]) or (gt_max + outlier_z) <= int(file_name[6:-4]): # 박테리아가 없는 z일 경우\n",
    "                    labels.append(0) # 라벨을 0로 지정\n",
    "\n",
    "    if validation == False:\n",
    "        train_len = int(round(len(labels) * train_val_rate))\n",
    "        labels = labels[:train_len]\n",
    "        \n",
    "    if validation == True:\n",
    "        train_len = int(round(len(labels) * train_val_rate))\n",
    "        labels = labels[train_len:]\n",
    "        \n",
    "    num_samples = len(labels) \n",
    "\n",
    "    class_counts = Counter(labels)\n",
    "\n",
    "    # 클래스별 가중치 계산\n",
    "    weights = [1.0 / class_counts[label] for label in labels] # 개수가 많은 라벨일 수록 적은 가중치가 적용됨\n",
    "    sampler = WeightedRandomSampler(weights, \n",
    "                                    num_samples= num_samples, \n",
    "                                    replacement=True) # 중복 샘플링 허용 \n",
    "\n",
    "    print(class_counts)\n",
    "\n",
    "    return sampler\n",
    "\n",
    "train_sampler = sampler(validation = False, train_val_rate = train_val_rate)\n",
    "val_sampler = sampler(validation = True, train_val_rate = train_val_rate)\n",
    "\n",
    "train_dataset = Classification_Dataset(root_dir='/kaggle/input/byu-locating-bacterial-flagellar-motors-2025', split='train', validation=False, train_val_rate=train_val_rate, base_transform=base_transform, aug_transform=aug_transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True, sampler=train_sampler)\n",
    "\n",
    "val_dataset = Classification_Dataset(root_dir='/kaggle/input/byu-locating-bacterial-flagellar-motors-2025', split='train', validation=True, train_val_rate=train_val_rate, base_transform=base_transform, aug_transform=aug_transform)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet 모델 로드 \n",
    "model = models.resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "\n",
    "# 첫 번째 Conv 레이어 수정 (in_channels=1)\n",
    "model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# output을 binary로 변경\n",
    "num_classes = 2\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "#  cpu 혹은 gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cd6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingAndCheckpoint:\n",
    "    def __init__(self, patience=5, checkpoint_path='model.pt'):\n",
    "        self.patience = patience            # 몇 번 참을지\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_train_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, val_loss, model):\n",
    "\n",
    "        improved = False\n",
    "\n",
    "        # 조건 1: 저장 조건 확인\n",
    "        if (self.best_val_loss > val_loss) and (self.best_train_loss > train_loss):\n",
    "            torch.save(model.state_dict(), self.checkpoint_path)\n",
    "            print(f\"Checkpoint saved! train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "            self.best_val_loss = val_loss\n",
    "            self.best_train_loss = train_loss\n",
    "            self.counter = 0\n",
    "            improved = True\n",
    "\n",
    "        # 조건 2: 조기 종료 조건\n",
    "        elif (val_loss > self.best_val_loss):\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement. train_loss={train_loss:.4f}, val_loss={val_loss:.4f}. Patience counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "        return improved\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, epochs, val_dataloader, early_stopper, scheduler):\n",
    "    \n",
    "    start = time() #학습이 얼마나 걸리는 지 확인 (시작)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_val_loss = 0.0\n",
    "        total_val_correct = 0\n",
    "        for idx, data in enumerate(tqdm(train_loader)):\n",
    "            model.train() \n",
    "            images, labels = data[0].to(device), data[1].to(device) \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.max(outputs, dim=1)[1] \n",
    "            total_correct += (preds == labels).sum() \n",
    "            \n",
    "            loss.backward()  \n",
    "            optimizer.step() \n",
    "            scheduler.step()\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            model.eval() # evaluation mode # batch norm과 drop out의 모드 전환. 그 외에는 동일\n",
    "            for data in tqdm(val_dataloader): \n",
    "                images, labels = data[0].to(device), data[1].to(device) \n",
    "                outputs = model(images) \n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.max(outputs, dim=1)[1] \n",
    "                total_val_correct +=  (preds == labels).sum() \n",
    "                \n",
    "        train_avg_loss = total_loss / len(train_loader)\n",
    "        train_avg_correct = total_correct / len(train_loader)\n",
    "\n",
    "        val_avg_loss = total_val_loss / len(val_dataloader)\n",
    "        val_avg_correct = total_val_correct / len(val_dataloader)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        wandb.log({\"train loss\": train_avg_loss, \"train accuracy\" : train_avg_correct, \"validation loss\": val_avg_loss, \"validation accuracy\" : val_avg_correct, \"learning rate\" : current_lr})\n",
    "            \n",
    "        improved = early_stopper(train_avg_loss, val_avg_loss, model)\n",
    "            \n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "        \n",
    "                \n",
    "    end = time() #학습이 얼마나 걸리는 지 확인 (끝)\n",
    "\n",
    "    print(\"Training Done.\")\n",
    "    print(f\"Elasped Time : {end-start:.4f} secs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"simple-rule-project\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"BYU - Locating Bacterial Flagellar Motors 2025\",\n",
    "    # Track hyperparameters and run metadata.\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\" : batch_size,\n",
    "        \"architecture\": \"resnet34-binary-classification\",\n",
    "        \"setting\": \"basic\",\n",
    "    },\n",
    ")\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.0001,                # 학습률 최고점\n",
    "    steps_per_epoch=len(train_dataloader),  # 한 에폭 안의 스텝 수\n",
    "    epochs=epochs,                 # 전체 epoch 수\n",
    "    pct_start=0.01,             # 얼마나 빨리 최고점에 도달할지 (30% 지점에서 최고)\n",
    "    anneal_strategy='cos',     # 감소 방식: 'cos' 또는 'linear'\n",
    "    final_div_factor=1e4       # 마지막 lr은 max_lr / final_div_factor\n",
    ")\n",
    "early_stopper = EarlyStoppingAndCheckpoint(\n",
    "    patience=5,\n",
    "    checkpoint_path='best_classification_model.pt',\n",
    ")\n",
    "\n",
    "train_model(model, criterion, optimizer, train_dataloader, epochs, val_dataloader, early_stopper, scheduler)\n",
    "\n",
    "wandb.save('best_classification_model.pt')\n",
    "wandb.finish()\n",
    "\n",
    "# 모델 로컬 다운\n",
    "os.chdir(r'/kaggle/working')\n",
    "FileLink(r'best_classification_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc3ce3",
   "metadata": {},
   "source": [
    "### 오버레이 두 번째 문제\n",
    "![image](/assets/images/BYU images/overlay2.png){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "**binary classification** 모델을 완성하고 나서 tomogram마다 오버레이의 결과가 **천차만별**이라는 것을 알게 됐습니다. 오버레이가 잘 적용되는 이미지도 있지만 위 tomogram처럼 오버레이의 결과가 안좋은 경우도 있습니다. 이를 해결하기 위해서는 **오버레이의 수학식**을 **일일이** tomogram마다 **조정**해주어야 하는데 이는 **불가능(무의미)**합니다. 결국, **binary classification** 모델이 오버레이할 이미지를 잘 골라낸다고 해도 오버레이한 결과가 tomogram마다 상이하기에 **binary classification** 모델 **무용지물**이 되는 것 입니다. 이를 통해 항상 아이디어가 있다면 다른 상황에서 **예외**가 발생하지 않는지 **검증**을 미리해야 한다는 것을 깨달았습니다.\n",
    "\n",
    "### U-Net 활용\n",
    "\n",
    "이미지를 오버레이 하기 쉽도록 **U-Net**으로 **배경을 제거**하는 방법도 생각해봤습니다.\n",
    "\n",
    "![image](/assets/images/BYU images/unet train data.png){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "**규칙 기반(rule-based)**으로 하얀 바탕 이미지 위에 **타원, 사각형, 곡선을 무작위로 생성**한 이미지를 만듭니다. 그리고 그 위에 **가우시안 노이즈**를 뿌린 이미지를 따로 만듭니다. 데이터가 모두 만들어지면 **노이즈를 뿌린 이미지를 U-Net의 입력 데이터로, 노이즈를 뿌리지 않은 이미지를 정답 데이터로 학습**합니다.\n",
    "\n",
    "![image](/assets/images/BYU images/unet output.png){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "U-Net 모델의 학습 결과는 위와 같습니다. 어느정도 배경을 제거하기는 했지만 아무래도 수학적인 방식이 아니라 인공지능 모델이 복원했기 때문에 편모와 박테리아만 잘 남길 수 있다고 **신뢰하기는 어렵습니다**. 게다가 U-Net이 꽤 큰 모델이기 때문에, 한 tomogram당 300~600장 정도되는 이미지를 모두 inference하기에는 **cost 측면에서 어려움**이 많다고 느꼈습니다. \n",
    "\n",
    "결과적으로, 여러가지 한계로 인해 **오버레이 방식은 사용할 수 없다고 판단**하였습니다.\n",
    "\n",
    "## 4. Yolo8 개조 - 다중 채널 입력\n",
    "\n",
    "오버레이로 여러 이미지를 한 장으로 만드는 대신, yolo8 모델을 개조해서 여러 장의 이미지를 입력으로 받을 수 있도록 하는 방향으로 전향했습니다. 즉, 한 tomogram에 대한 **채널 사이즈가 1인 흑백 이미지들을 채널 방향으로 여러 장 쌓아서 하나의 데이터**를 이루도록 했습니다.\\\n",
    "그런데 한 tomogram에 대한 모든 이미지(slice)를 합치지 않고 **8장 간격**으로 **5장**의 이미지만 합쳤습니다 (e.g. 0, 8, 16, 24, 32). 그 이유는 뒤에 설명할 **Multi-head** 방식에서 **segmentation head**가 **1 채널**을 입력으로 받기 때문에 segmentation head의 입력과 너무 다른 입력 데이터가 들어오는 것을 방지하기 위함입니다.\n",
    "\n",
    "yolo8 모델이 여러 장의 이미지를 입력으로 받을 수 있도록 ultralytics에서 제공하는 yolo8의 내부 코드를 직접 뜯어보고 개조하는 과정을 거쳤습니다. 그러나 yolo8이 여러 장의 이미지를 한 번에 받을 수 있게 되어도 여전히 (x, y) 좌표만 예측할 뿐 **z** 값까지는 예측하지 못했습니다. 그래서 **z를 예측할 수 있는 모델**을 따로 만들었습니다. 즉, x, y를 예측하는 yolo8 모델과 z를 예측하는 모델이 따로 존재합니다.\\\n",
    "yolo 모델이 출력한 x, y 좌표값을 중심으로 일정 범위만큼 이미지를 **잘라(crop)냅니다**. 그렇게 구한 더 작은 height, width를 가진 5채널 이미지를 z를 예측하는 모델에 주어 z값을 에측하게 됩니다. z를 예측하는 모델로, **attention layer**에 **ANN layer**를 결합한 모델과 **ResNet**에 **ANN layer**를 결합한 모델을 비교해 보았습니다. 그 결과 **ResNet**에 **ANN layer**를 결합한 모델의 성능이 더 뛰어났습니다.\\\n",
    "하지만 **yolo가 좌표를 잘못 예측**하게 되면 잘라낸 이미지에 모터의 이미지가 담겨있지 않음으로 절대 **z의 좌표를 제대로 찾을 수 없게 되는 방식**입니다.\n",
    "\n",
    "## 5. Multi-head Yolo8\n",
    "![image](/assets/images/BYU images/multi-head.png){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "yolo8에는 모터의 좌표를 탐지하는 **Detection head**가 기본적으로 달려있습니다. 그런데 추가로 몸체, 편모, 모터를 모두 예측하는 **Segmentation head**를 달아서 둘을 동시에 학습시키면 모델의 성능이 더 좋아질 것이라고 가정을 했습니다. \n",
    "\n",
    "![image](/assets/images/BYU images/segment data.png){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "그러나 주어진 데이터에는 segment를 위한 **라벨링**이 되어 있지 않기 때문에 제가 직접 규칙 기반(Rule-based)으로 **segmentation 데이터를 생성**하기로 했습니다. 위 이미지가 규칙 기반으로 만들어진 데이터 입니다. 각 이미지는 몸통, 편모, 모터에 대한 **segmentation mask**와 **bbox**가 함께 출력됩니다. \n",
    "\n",
    "**Detection head**는 입력으로 5채널을 받고 **Segmentation head**는 입력으로 1채널을 입력으로 받기 때문에 입력(CNN) 레이어도 Multi-head와 동일하게 **2개**가 존재합니다.\n",
    "\n",
    "학습을 할 때만 **Multi-head**를 사용하고 학습이 완료되면 **Detection head**만 사용하게 됩니다.\n",
    "\n",
    "아래가 **Multi-head yolo8** 모델을 학습시키는 코드입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54234b2e",
   "metadata": {},
   "source": [
    "### 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torchvision.models as models\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.special import comb\n",
    "\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "'''\n",
    "# detection label format\n",
    "batch = {\n",
    "    \"batch_idx\": tensor([0, 0, 1]),  # 3개의 GT가 있고, 2개는 이미지 0에, 1개는 이미지 1에 속함\n",
    "    \"cls\": tensor([2, 5, 1]),        # 각각 클래스 2, 5, 1\n",
    "    \"bboxes\": tensor([\n",
    "        [0.1, 0.1, 0.2, 0.2], # xywh\n",
    "        [0.3, 0.3, 0.5, 0.5],\n",
    "        [0.4, 0.4, 0.6, 0.6]\n",
    "    ])\n",
    "}\n",
    "\n",
    "# segmentation label format\n",
    "batch = {\n",
    "    \"batch_idx\": tensor([0, 0, 1, 2, 2]), # mask 제외 Detection과 동일\n",
    "    \"cls\":       tensor([3, 7, 5, 1, 1]),\n",
    "    \"bboxes\":    tensor([\n",
    "                    [0.5, 0.5, 0.3, 0.4],\n",
    "                    [0.2, 0.3, 0.2, 0.3],\n",
    "                    [0.4, 0.4, 0.5, 0.5],\n",
    "                    [0.6, 0.6, 0.1, 0.1],\n",
    "                    [0.3, 0.2, 0.2, 0.3],\n",
    "                 ]),\n",
    "    \"masks\":     tensor([5, 160, 160])  # 각 마스크는 객체별로 2D 바이너리\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939011f2",
   "metadata": {},
   "source": [
    "### Train  Dataset, Dataloader 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1281c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detect3D_Train_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir = \"\", validation=False, train_data_rate=1):\n",
    "        super(Detect3D_Train_Dataset, self).__init__() # 상속\n",
    "\n",
    "        self.data_list = []  # self.data_list에 모든 파일 목록 저장\n",
    "\n",
    "        self.images_directory = os.path.join(root_dir, \"images\")\n",
    "        self.labels_directory = os.path.join(root_dir, \"labels\")\n",
    "        \n",
    "        for sub_directory in os.listdir(self.images_directory):\n",
    "            full_directory = os.path.join(self.images_directory, sub_directory)\n",
    "            for file_name in os.listdir(full_directory):\n",
    "                label_name = file_name[:-4] + \".txt\"\n",
    "                try:\n",
    "                    with open(f\"{self.labels_directory}/{sub_directory }/{label_name}\", \"r\") as f:\n",
    "                        labels = f.read()\n",
    "                    self.data_list.append(os.path.join(full_directory, file_name) + \"|\" + labels)\n",
    "                except FileNotFoundError:\n",
    "                    self.data_list.append(os.path.join(full_directory, file_name) + \"|\")\n",
    "\n",
    "        self.data_list = sorted(self.data_list)\n",
    "\n",
    "        self.validation = validation\n",
    "        self.train_rate = train_data_rate\n",
    "\n",
    "\n",
    "        train_len = int(round(len(self.data_list) * self.train_rate))\n",
    "        if self.validation == False:\n",
    "            self.data_list = self.data_list[:train_len]\n",
    "        else:\n",
    "            self.data_list = self.data_list[train_len:]\n",
    "\n",
    "    def transform(self, img: torch.Tensor, bboxes: torch.Tensor):\n",
    "        \"\"\"\n",
    "        img: (5, H, W) - 5채널 이미지\n",
    "        bboxes: (N, 4) - xywh 정규화 (0~1 범위)\n",
    "        Returns: transformed img, transformed bboxes (정규화 유지)\n",
    "        \"\"\"\n",
    "        #_, H, W = img.shape\n",
    "        transform_type = random.choice(['none', 'hflip', 'vflip', 'rot90', 'rot180', 'rot270'])\n",
    "    \n",
    "        bboxes_clone = bboxes.clone()\n",
    "    \n",
    "        if transform_type == 'none':\n",
    "            return img, bboxes\n",
    "    \n",
    "        if transform_type == 'hflip':\n",
    "            img = torch.flip(img, dims=[2])  # W axis\n",
    "            bboxes[:, 0] = 1.0 - bboxes_clone[:, 0]\n",
    "        elif transform_type == 'vflip':\n",
    "            img = torch.flip(img, dims=[1])  # H axis\n",
    "            bboxes[:, 1] = 1.0 - bboxes_clone[:, 1]\n",
    "        elif transform_type == 'rot90':\n",
    "            img = torch.rot90(img, k=1, dims=[1, 2])\n",
    "            x, y, w, h = bboxes_clone[:, 0], bboxes_clone[:, 1], bboxes_clone[:, 2], bboxes_clone[:, 3]\n",
    "            bboxes[:, 0] = y\n",
    "            bboxes[:, 1] = 1.0 - x\n",
    "            bboxes[:, 2] = h\n",
    "            bboxes[:, 3] = w\n",
    "        elif transform_type == 'rot180':\n",
    "            img = torch.rot90(img, k=2, dims=[1, 2])\n",
    "            bboxes[:, 0] = 1.0 - bboxes_clone[:, 0]\n",
    "            bboxes[:, 1] = 1.0 - bboxes_clone[:, 1]\n",
    "        elif transform_type == 'rot270':\n",
    "            img = torch.rot90(img, k=3, dims=[1, 2])\n",
    "            x, y, w, h = bboxes_clone[:, 0], bboxes_clone[:, 1], bboxes_clone[:, 2], bboxes_clone[:, 3]\n",
    "            bboxes[:, 0] = 1.0 - y\n",
    "            bboxes[:, 1] = x\n",
    "            bboxes[:, 2] = h\n",
    "            bboxes[:, 3] = w\n",
    "    \n",
    "        return img, bboxes\n",
    "        \n",
    "\n",
    "    def __len__(self): # 데이터의 개수 # batch단위로 자르기 위함\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path_label = self.data_list[index]\n",
    "\n",
    "        image_path, label = image_path_label.split(\"|\")[0], image_path_label.split(\"|\")[1]\n",
    "        \n",
    "        image = np.load(image_path)['arr_0']\n",
    "        image = torch.tensor(image / 255).float()\n",
    "        \n",
    "        if label != \"\":\n",
    "            label = label.strip().split()\n",
    "            z = torch.tensor([int(label[i:i+5][-1][:-2]) for i in range(0, len(label), 5)], dtype=torch.uint8)\n",
    "            bboxes = torch.tensor(np.array([label[i:i+5][:4] for i in range(0, len(label), 5)], dtype=np.float64))\n",
    "            image, bboxes = self.transform(image, bboxes)\n",
    "            label_dict = {\"bboxes\": bboxes, \"z\": z}\n",
    "\n",
    "        elif label == \"\":\n",
    "            label_dict = {\"bboxes\":[], \"z\":[]}\n",
    "\n",
    "        return image, label_dict\n",
    "    \n",
    "def collate_fn(batch): # Yolov8의 loss function이 요구하는 형식에 맞게 dataloader의 출력을 변경\n",
    "    images = [x[0] for x in batch]\n",
    "    labels = [x[1] for x in batch]\n",
    "\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    batch_idx = [([i] * len(x[\"bboxes\"])) for i, x in enumerate(labels)]\n",
    "    batch_idx = torch.tensor(np.concatenate(batch_idx), dtype=torch.int16)\n",
    "    cls = [([0] * len(x[\"bboxes\"])) for x in labels]\n",
    "    cls = torch.tensor(np.concatenate(cls), dtype=torch.int16)\n",
    "    try:\n",
    "        bboxes = torch.cat([x[\"bboxes\"] for x in labels if len(x[\"bboxes\"]) > 0], dim=0)\n",
    "        z = torch.cat([x[\"z\"] for x in labels if len(x[\"z\"]) > 0], dim=0).type(torch.uint8)\n",
    "    except: # 라벨이 없는 경우\n",
    "        bboxes = torch.tensor([], dtype=torch.float32)\n",
    "        z = torch.tensor([], dtype=torch.uint8)\n",
    "    return images, {\"batch_idx\":batch_idx, \"cls\":cls, \"bboxes\":bboxes, \"z\":z}\n",
    "\n",
    "def sampler(root_dir = \"\", validation = False, train_data_rate = 1): # class unbalance를 해소하기 위한 가중 샘플링\n",
    "    from collections import Counter\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    images_directory = os.path.join(root_dir, \"images\")\n",
    "    labels_directory = os.path.join(root_dir, \"labels\")\n",
    "    for sub_directory in os.listdir(images_directory):\n",
    "        images_full_directory = os.path.join(images_directory, sub_directory)\n",
    "        for file_name in os.listdir(images_full_directory):\n",
    "            label_name = file_name[:-4] + \".txt\"\n",
    "            if os.path.isfile(f\"{labels_directory}/{sub_directory }/{label_name}\"):\n",
    "                labels.append(f\"{sub_directory}/{file_name}/1\")\n",
    "            else:\n",
    "                labels.append(f\"{sub_directory}/{file_name}/0\")\n",
    "\n",
    "\n",
    "    labels = sorted(labels)\n",
    "    labels = [int(x[-1]) for x in labels]\n",
    "\n",
    "    if validation == False:\n",
    "        train_len = int(round(len(labels) * train_data_rate))\n",
    "        labels = labels[:train_len]\n",
    "\n",
    "    else:\n",
    "        train_len = int(round(len(labels) * train_data_rate))\n",
    "        labels = labels[train_len:]\n",
    "\n",
    "    num_samples = len(labels)\n",
    "\n",
    "    class_counts = Counter(labels)\n",
    "\n",
    "    # 클래스별 가중치 계산\n",
    "    weights = [1.0 / class_counts[label] for label in labels] # 개수가 많은 라벨일 수록 적은 가중치가 적용됨\n",
    "    sampler = WeightedRandomSampler(weights,\n",
    "                                    num_samples= num_samples,\n",
    "                                    replacement=True) # 중복 샘플링 허용\n",
    "\n",
    "    print(class_counts)\n",
    "\n",
    "    return sampler\n",
    "\n",
    "train_data_rate = 0.8\n",
    "batch_size = 18 # 18\n",
    "root_dir = \"/kaggle/input/my-dataset\"\n",
    "\n",
    "def data_loader_process(root_dir, train_data_rate, batch_size):\n",
    "    train_sampler= sampler(root_dir = root_dir, validation = False, train_data_rate = train_data_rate)\n",
    "    val_sampler = sampler(root_dir = root_dir, validation = True, train_data_rate = train_data_rate)\n",
    "    \n",
    "    train_set = Detect3D_Train_Dataset(root_dir = root_dir, validation=False, train_data_rate=train_data_rate)\n",
    "    val_set = Detect3D_Train_Dataset(root_dir = root_dir, validation=True, train_data_rate=train_data_rate)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=collate_fn, pin_memory=True, sampler=train_sampler)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=collate_fn, pin_memory=True, sampler=val_sampler)\n",
    "\n",
    "    print(\"Train data 개수 :\",len(train_set), \"\\n\", \"Validation data 개수 :\", len(val_set),\n",
    "          \"\\n\", \"Train batch 개수 :\", len(train_loader), \"\\n\", \"Validation batch 개수 :\", len(val_loader))\n",
    "    return train_loader, val_loader\n",
    "    \n",
    "train_loader, val_loader = data_loader_process(root_dir, train_data_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e2242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Make_bacteria_data:\n",
    "    def __init__(self):\n",
    "\n",
    "        # must match height and width \n",
    "        self.img_H = 640    \n",
    "        self.img_W = 640\n",
    "        \n",
    "        self.background_color = random.randint(190, 210) # 여러 개의 박테리아 이미지를 합치기 위해 박테리아의 색보다 배경 색이 무조건 더 커야함.\n",
    "        self.body_color = random.randint(140, 160)\n",
    "        self.border_color = random.randint(0, 20)\n",
    "        self.flagella_color = random.randint(60, 80)\n",
    "\n",
    "        self.base_body_size = 35\n",
    "\n",
    "    def get_bbox_from_mask(self, mask: np.ndarray):\n",
    "        \"\"\"\n",
    "        mask: np.ndarray of shape (H, W), binary mask with 0 (background) and 1 (object)\n",
    "        return: (x, y, w, h) if object exists, else None\n",
    "        \"\"\"\n",
    "        if mask.sum() == 0:\n",
    "            return None  # No object in mask\n",
    "\n",
    "        y_indices, x_indices = np.where(mask == 1)\n",
    "\n",
    "        xmin = x_indices.min()\n",
    "        xmax = x_indices.max()\n",
    "        ymin = y_indices.min()\n",
    "        ymax = y_indices.max()\n",
    "\n",
    "        w = xmax - xmin + 1\n",
    "        h = ymax - ymin + 1\n",
    "\n",
    "        x_center = xmin + w / 2\n",
    "        y_center = ymin + h / 2\n",
    "\n",
    "        return [x_center, y_center, w, h]\n",
    "\n",
    "    def make_body_points(self, center_point):\n",
    "        # center_point, second_point == np.array([x, y])\n",
    "        result = []\n",
    "        random = np.random.randint(-1, 2, (1, 2))\n",
    "        second_point = center_point + random\n",
    "        result.append(center_point)\n",
    "        result.append(second_point)\n",
    "        for i in range(30):\n",
    "            random = np.random.randint(-1, 2, (1, 2))\n",
    "            result.append(result[-1] - (result[-2] - result[-1]) + random)\n",
    "\n",
    "        return np.array(result)\n",
    "\n",
    "    def bezier_curve(self, control_points, num_points=100):\n",
    "        n = len(control_points) - 1\n",
    "        ts = np.linspace(0, 1, num=num_points)\n",
    "        curve = []\n",
    "\n",
    "        for t in ts:\n",
    "            point = np.zeros(2)\n",
    "            for i in range(n + 1):\n",
    "                bernstein = comb(n, i) * (1 - t)**(n - i) * t**i\n",
    "                point += bernstein * control_points[i]\n",
    "            curve.append(point)\n",
    "\n",
    "        return np.array(curve, dtype= np.uint64)\n",
    "\n",
    "    def perpendicular_points(self, p1, p2):\n",
    "        \"\"\"\n",
    "        p1, p2: 두 점의 좌표 (튜플 또는 리스트) (x, y)\n",
    "        distance: 수직선 상에서 교차 지점으로부터 떨어진 거리\n",
    "        \"\"\"\n",
    "        # 벡터 방향 (p1 -> p2)\n",
    "        direction = (p2 - p1)\n",
    "\n",
    "        # 직각 방향 벡터 얻기 (90도 회전)\n",
    "        perp_direction = np.array([-direction[1], direction[0]])\n",
    "\n",
    "        # 정규화하여 단위 벡터로 만들기\n",
    "        perp_unit = perp_direction / np.linalg.norm(perp_direction)\n",
    "\n",
    "        distance = np.random.randint(-100, 100)\n",
    "\n",
    "        # 거리만큼 떨어진 두 점 구하기\n",
    "        point1 = p2 + perp_unit * distance\n",
    "        point1 = np.clip(point1, 0, self.img_H)\n",
    "\n",
    "        return point1.astype(np.uint64).tolist() #point2.tolist()\n",
    "\n",
    "    def make_body_mask(self):\n",
    "\n",
    "        num_bodys = np.random.choice([1, 2, 3], p=[0.8, 0.15, 0.05])\n",
    "        if num_bodys == 3:\n",
    "            tri_up_x, tri_up_y = int(self.img_W / 2), int(self.img_H / 3) - 30\n",
    "            tri_left_x, tri_left_y = int(self.img_W / 3) - 30, int(self.img_H / 3*2) + 30 \n",
    "            tri_right_x, tri_right_y = int(self.img_W / 3*2) + 30, int(self.img_H / 3*2) + 30\n",
    "            center_points = np.array([[tri_up_x, tri_up_y], [tri_left_x, tri_left_y], [tri_right_x, tri_right_y]]).reshape(-1, 1, 2)\n",
    "        elif num_bodys == 2:\n",
    "            double_up_x, double_up_y = int(self.img_W / 3), int(self.img_H / 3)\n",
    "            double_down_x, double_down_y = int(self.img_W / 3*2), int(self.img_H / 3*2)\n",
    "            center_points = np.array([[double_up_x, double_up_y], [double_down_x, double_down_y]]).reshape(-1, 1, 2)\n",
    "        elif num_bodys == 1:\n",
    "            center_points = np.array([[int(self.img_H/2), int(self.img_W/2)]]).reshape(-1, 1, 2)\n",
    "\n",
    "        body_masks = []\n",
    "        body_bboxes = []\n",
    "        for center_point in center_points:\n",
    "            \n",
    "            mask = np.zeros((self.img_H, self.img_W), dtype=np.uint8)\n",
    "\n",
    "            body_points = self.make_body_points(center_point)\n",
    "\n",
    "            body_weight = random.randint(0, 2)\n",
    "            if body_weight == 0:\n",
    "                r_weight = np.ones(len(body_points)).astype(np.uint8)\n",
    "            if body_weight == 1:\n",
    "                r_weight = (np.arange(0, len(body_points)) / 12 + 1).astype(np.uint8)\n",
    "            if body_weight == 2:\n",
    "                r_weight = (np.arange(0, len(body_points))[::-1] / 12 + 1).astype(np.uint8)\n",
    "\n",
    "            for j, x in enumerate(body_points):\n",
    "                cv2.circle(mask, x[0], self.base_body_size * r_weight[j], 1, -1)\n",
    "\n",
    "            body_masks.append(mask)\n",
    "\n",
    "            body_bboxes.append(self.get_bbox_from_mask(mask))\n",
    "\n",
    "        return body_masks, body_bboxes, body_weight\n",
    "\n",
    "    def mask_body_border(self, mask, body_weight):\n",
    "\n",
    "        img_H, img_W = mask.shape\n",
    "\n",
    "        # 경계선: erosion 후 원래 마스크와 차이 계산\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        eroded = cv2.erode(mask, kernel, iterations=1)\n",
    "        inner_border = mask - eroded  # 테두리만 남음\n",
    "\n",
    "        if body_weight > 0:\n",
    "            boader_dist = random.randint(5, 10)\n",
    "        else:\n",
    "            boader_dist = 5\n",
    "        dilated1 = cv2.dilate(mask, kernel, iterations=boader_dist)\n",
    "        dilated2 = cv2.dilate(mask, kernel, iterations=boader_dist+1)\n",
    "        outer_border = dilated2 - dilated1 - mask    # 테두리만 남음\n",
    "\n",
    "        # 시각화용 결과 이미지 (흰색 배경)\n",
    "        \n",
    "        image = np.ones((img_H, img_W), dtype=np.uint8) * self.background_color\n",
    "   \n",
    "        image[mask == 1] = self.body_color        # 도형 내부를 회색으로\n",
    "        image[inner_border == 1] = self.border_color         # 경계선은 검정으로\n",
    "        image[outer_border == 1] = self.border_color         # 경계선은 검정으로\n",
    "        \n",
    "        return image, inner_border, outer_border, boader_dist\n",
    "\n",
    "    def make_flagella_and_mask(self, body_image, inner_border, outer_border, boader_dist):\n",
    "        inner_border_point = np.argwhere(inner_border == 1)\n",
    "        inner_border_point_randidx = np.random.randint(inner_border_point.shape[0])\n",
    "        inner_border_randpoint = inner_border_point[inner_border_point_randidx]\n",
    "\n",
    "        outer_border_point = np.argwhere(outer_border == 1)\n",
    "        distances = np.linalg.norm(outer_border_point - inner_border_randpoint, axis=1) # 거리 계산 (유클리드 거리)\n",
    "        closest_idx = np.argmin(distances) # 가장 가까운 인덱스\n",
    "        closest_point = outer_border_point[closest_idx]\n",
    "\n",
    "        dirction = (inner_border_randpoint - closest_point)    \n",
    "        dirction_unit = dirction / np.linalg.norm(dirction)\n",
    "        point3 = closest_point - 10 * dirction_unit\n",
    "        point4 = self.perpendicular_points(point3, closest_point - 30 * dirction_unit)\n",
    "        point5 = self.perpendicular_points(point3, closest_point - 50 * dirction_unit)\n",
    "        point6 = self.perpendicular_points(point3, closest_point - 70 * dirction_unit)\n",
    "\n",
    "        control_points = np.array([list(inner_border_randpoint), list(closest_point), list(point3), list(point4), list(point5), list(point6)])\n",
    "        curve_points = self.bezier_curve(control_points, num_points=100)\n",
    "        curve_points = curve_points.reshape(-1,1,2)[:,:,::-1]\n",
    "        \n",
    "        valid_mask = np.all((curve_points >= 0) & (curve_points <= self.img_H), axis=(1, 2))\n",
    "        curve_points = curve_points[valid_mask] # 조건을 만족하는 원소만 추출\n",
    "\n",
    "        \n",
    "        flagella_thickness = 1 * boader_dist - 2\n",
    "        cv2.polylines(body_image, [curve_points], isClosed=False, color=self.flagella_color, thickness=flagella_thickness)\n",
    "\n",
    "        flagella_mask = np.array(cv2.polylines(np.zeros((self.img_H, self.img_W), dtype=np.uint8), [curve_points.astype(np.int32)], isClosed=False, color=1, thickness=flagella_thickness))\n",
    "        flagella_bbox = self.get_bbox_from_mask(flagella_mask)\n",
    "\n",
    "        motor_mask = np.array(cv2.circle(np.zeros((self.img_H, self.img_W)), inner_border_randpoint[::-1], 5, 1, -1), dtype=np.uint8)\n",
    "        motor_bbox = self.get_bbox_from_mask(motor_mask)\n",
    "\n",
    "        return body_image, flagella_mask, flagella_bbox, motor_mask, motor_bbox\n",
    "\n",
    "    def make_image(self):\n",
    "        body_masks, body_bboxes, body_weight = self.make_body_mask()\n",
    "        flagella_masks = []\n",
    "        flagella_bboxes = []\n",
    "        motor_masks = []\n",
    "        motor_bboxes = []\n",
    "        images = []\n",
    "\n",
    "        for i in body_masks:\n",
    "            image,  inner_border, outer_border, boader_dist = self.mask_body_border(i, body_weight)\n",
    "            \n",
    "            num_flagella = np.random.choice([0, 1, 2, 3], p=[0.3, 0.4, 0.2, 0.1])\n",
    "            for j in range(num_flagella):\n",
    "                image, flagella_mask, flagella_bbox, motor_mask, motor_bbox= self.make_flagella_and_mask(image, inner_border, outer_border, boader_dist) \n",
    "                flagella_masks.append(flagella_mask)\n",
    "                flagella_bboxes.append(flagella_bbox)\n",
    "                motor_masks.append(motor_mask)\n",
    "                motor_bboxes.append(motor_bbox)\n",
    "                \n",
    "            images.append(image)\n",
    "\n",
    "        if len(body_masks) == 1:\n",
    "            image = images[0]\n",
    "        if len(body_masks) > 1:\n",
    "            image = np.where(images[0] < self.background_color, images[0], images[1])\n",
    "        if len(body_masks) > 2:\n",
    "            image = np.where(image < self.background_color, image, images[2])\n",
    "\n",
    "        cls = [] # body: 0, flagella: 1, motor: 2\n",
    "        masks = []\n",
    "        bboxes = []\n",
    "\n",
    "        for i, j in zip(body_masks, body_bboxes):\n",
    "            masks.append(i)\n",
    "            bboxes.append(j)\n",
    "            cls.append(0)\n",
    "\n",
    "        for i, j in zip(flagella_masks, flagella_bboxes):\n",
    "            masks.append(i)\n",
    "            bboxes.append(j)\n",
    "            cls.append(1)\n",
    "\n",
    "        for i, j in zip(motor_masks, motor_bboxes):\n",
    "            masks.append(i)\n",
    "            bboxes.append(j)\n",
    "            cls.append(2)\n",
    "\n",
    "        cls = torch.tensor(cls)\n",
    "        masks = torch.tensor(np.array(masks))\n",
    "        bboxes = np.array(bboxes)\n",
    "        bboxes[:,[0, 2]] = bboxes[:,[0, 2]] / self.img_W # width\n",
    "        bboxes[:,[1, 3]] = bboxes[:,[1, 3]] / self.img_H # height\n",
    "        bboxes = torch.tensor(bboxes)\n",
    "\n",
    "        return image, cls, masks, bboxes\n",
    "\n",
    "    def add_noise(self, image):\n",
    "        # 가우시안 노이즈 추가\n",
    "        mean = 0\n",
    "        std = random.randint(70, 80)  # 표준편차, 노이즈 세기\n",
    "        noise = np.random.normal(mean, std, image.shape)\n",
    "\n",
    "        noisy_img = image + noise\n",
    "        noisy_img = np.clip(noisy_img, 0, 255).astype(np.uint8)\n",
    "\n",
    "        return noisy_img\n",
    "\n",
    "    def add_noise_ellipse(self, image):\n",
    "        # 타원 그리기\n",
    "        num_ellipse = random.randint(0, 30)\n",
    "        color = random.randint(20, 70)\n",
    "\n",
    "        for _ in range(num_ellipse):\n",
    "            center = np.random.randint(0, self.img_H, (2))\n",
    "            axes = np.random.randint(1, 8, (2))          # 반가로: 100, 반세로: 50\n",
    "            angle = random.randint(0, 90)                # 회전 각도\n",
    "\n",
    "            cv2.ellipse(image, center, axes, angle, 0, 360, color=50, thickness=-1)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def __call__(self):\n",
    "        image, cls, masks, bboxes = self.make_image()\n",
    "        image = self.add_noise(image)\n",
    "        image = self.add_noise_ellipse(image)\n",
    "        image = torch.tensor(image)\n",
    "\n",
    "        return image, cls, masks, bboxes\n",
    "\n",
    "    def batch_generate(self, batch_size=8):\n",
    "        image_lst = []\n",
    "        cls_lst = []\n",
    "        masks_lst = []\n",
    "        bboxes_lst = []\n",
    "        batch_idx = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            image, cls, masks, bboxes = make_bacteria()\n",
    "            image_lst.append(image.unsqueeze(0) / 255)\n",
    "            cls_lst.append(cls)\n",
    "            masks_lst.append(masks)\n",
    "            bboxes_lst.append(bboxes)\n",
    "            batch_idx.append(torch.zeros(len(cls)) + i)\n",
    "        \n",
    "        batch_idx = torch.cat(batch_idx)\n",
    "        image_lst = torch.stack(image_lst, dim=0)\n",
    "        cls_lst = torch.cat(cls_lst)\n",
    "        bboxes_lst = torch.cat(bboxes_lst, dim=0)\n",
    "        masks_lst = torch.cat(masks_lst, dim=0)\n",
    "        \n",
    "        label = {\n",
    "            \"batch_idx\" : batch_idx,\n",
    "            \"cls\" : cls_lst,\n",
    "            \"bboxes\" : bboxes_lst,\n",
    "            \"masks\": masks_lst\n",
    "             }\n",
    "        \n",
    "        return image_lst, label\n",
    "    \n",
    "    def image_show(self, figsize=(6,6)):\n",
    "        import matplotlib.pyplot as plt\n",
    "        image, _, _, _ = self.__call__()\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Make_bacteria_data()\n",
    "a.image_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a95a6a8",
   "metadata": {},
   "source": [
    "![image](/assets/images/BYU images/segment data.png){: width=\"100%\" height=\"100%\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd5dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_z_label(data):\n",
    "    if data[1][\"batch_idx\"].numel() != 0:\n",
    "        crop_size = 80\n",
    "        crop_half_size = round(crop_size / 2)\n",
    "        cropped_images = []\n",
    "        vit_labels = []\n",
    "        batch_idx = data[1]['batch_idx']\n",
    "        for i in range(len(batch_idx)):\n",
    "            z = (data[1][\"z\"][i] / 3).int()\n",
    "            vit_labels.append(z)\n",
    "\n",
    "            bbox = data[1][\"bboxes\"][i].clone()\n",
    "            image = data[0][batch_idx[i].item()].clone()\n",
    "            img_height, img_width = 640, 640\n",
    "            x_center, y_center, _, _ = bbox\n",
    "            x_center *= img_width\n",
    "            y_center *= img_height\n",
    "            x_center, y_center = int(x_center), int(y_center)\n",
    "            \n",
    "            y_min = y_center-crop_half_size\n",
    "            y_max = y_center+crop_half_size\n",
    "            x_min = x_center-crop_half_size\n",
    "            x_max = x_center+crop_half_size\n",
    "            \n",
    "            if (y_min<0) | (y_max>640) | (x_min<0) | (x_max>640):\n",
    "                \n",
    "                back_ground = torch.zeros(5, crop_size, crop_size) + 0.5411\n",
    "                y_min_attach_idx, y_max_attach_idx, x_min_attach_idx, x_max_attach_idx = 0, crop_size, 0, crop_size\n",
    "                \n",
    "                if y_min < 0:\n",
    "                    y_min_attach_idx = abs(y_min)\n",
    "                    y_min = 0\n",
    "                if y_max > 640:\n",
    "                    y_max_attach_idx = crop_size - (y_max - 640)\n",
    "                    y_max = 640\n",
    "                if x_min < 0:\n",
    "                    x_min_attach_idx = abs(x_min)\n",
    "                    x_min = 0\n",
    "                if x_max > 640:\n",
    "                    x_max_attach_idx = crop_size - (x_max - 640)\n",
    "                    x_max = 640\n",
    "                    \n",
    "                image = image[:, y_min:y_max, x_min:x_max]\n",
    "                back_ground[:, y_min_attach_idx:y_max_attach_idx, x_min_attach_idx:x_max_attach_idx] = image\n",
    "                image = back_ground\n",
    "                \n",
    "            else:\n",
    "                image = image[:, y_min:y_max , x_min:x_max]\n",
    "                \n",
    "            cropped_images.append(image)\n",
    "        cropped_images = torch.stack(cropped_images, dim=0).type(torch.float)\n",
    "        vit_labels = torch.stack(vit_labels, dim=0).type(torch.int64)\n",
    "    else:\n",
    "        cropped_images = None\n",
    "        vit_labels = None\n",
    "        \n",
    "    return cropped_images, vit_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images, vit_labels = make_z_label(data)\n",
    "\n",
    "batch_index = 2 \n",
    "imgs = []\n",
    "for i in range(5):\n",
    "    imgs.append(cropped_images[batch_index][i])\n",
    "print(\"정답 라벨에 가장 가까운 이미지 :\", int((vit_labels[batch_index] / 8 + 0.1).round() +1), \"번째\")\n",
    "print(\"진짜 정답 라벨 :\", int(vit_labels[batch_index] + 1), \"/33 번째\")\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(12,12))\n",
    "\n",
    "#plt.axis('off')\n",
    "for i in range(5):\n",
    "    axes[i].imshow(imgs[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b112fa9a",
   "metadata": {},
   "source": [
    "![image](/assets/images/BYU images/crop.png){: width=\"100%\" height=\"100%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f71c99",
   "metadata": {},
   "source": [
    "### 모델 구현\n",
    "\n",
    "**Yolov8 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "# -----------------------------\n",
    "# Utility Modules\n",
    "# -----------------------------\n",
    "def make_anchors(feats, strides, grid_cell_offset=0.5):  # x, self.stride, 0.5\n",
    "    \"\"\"Generate anchors from features.\"\"\"\n",
    "    anchor_points, stride_tensor = [], []\n",
    "    assert feats is not None\n",
    "    dtype, device = feats[0].dtype, feats[0].device\n",
    "    for i, stride in enumerate(strides): # [8.0, 16.0, 32.0]\n",
    "        h, w = feats[i].shape[2:] if isinstance(feats, list) else (int(feats[i][0]), int(feats[i][1])) # size별로 feature의 h,w를 가져옴\n",
    "        sx = torch.arange(end=w, device=device, dtype=dtype) + grid_cell_offset  # shift x  # 0.5 ~ 79.5\n",
    "        sy = torch.arange(end=h, device=device, dtype=dtype) + grid_cell_offset  # shift y  # 0.5 ~ 79.5\n",
    "        sy, sx = torch.meshgrid(sy, sx, indexing=\"ij\") #if TORCH_1_10 else torch.meshgrid(sy, sx) # (w x h) size의 0.5 ~ 79.5로 채워진 행렬 2개\n",
    "        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2)) # 마지막에 차원을 하나 추가해서 해당 차원으로 sx,sy를 쌓음-> (80,80,2). 마지막 차원을 유지하고 2차원으로 만듬->(6400,2)\n",
    "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device)) # stride로 채워진 ((h*w), 1)크기의 텐서 생성 # 8:6400개, 16:1600개, 32:400개 -> (8400, 1)\n",
    "    return torch.cat(anchor_points), torch.cat(stride_tensor) \n",
    "        # torch.cat(anchor_points): (6400,2), (1600,2), (400,2)를 0방향으로 concat하여 (8400,2)\n",
    "        # torch.cat(stride_tensor): (6400,1), (1600,1), (400,1)를 0방향으로 concat하여 (8400,1)\n",
    "\n",
    "class DFL(nn.Module):\n",
    "    \"\"\"\n",
    "    Integral module of Distribution Focal Loss (DFL).\n",
    "\n",
    "    Proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c1=16):\n",
    "        \"\"\"Initialize a convolutional layer with a given number of input channels.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)\n",
    "        x = torch.arange(c1, dtype=torch.float) # [0.0, 1.0, ..., 15.0] 정수를 (학습하지 않는)파라미터로\n",
    "        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1)) \n",
    "        self.c1 = c1\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply the DFL module to input tensor and return transformed output.\"\"\"\n",
    "        b, _, a = x.shape  # batch, channels, anchors\n",
    "        return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a) #softmax를 확률값으로 하여 [0.0, 1.0, ..., 15.0]의 기대값을 구함\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model Modules\n",
    "# -----------------------------\n",
    "def autopad(k, p=None):\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]\n",
    "    return p\n",
    "    \n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, act=True):\n",
    "        super().__init__()\n",
    "        c1, c2 = int(c1), int(c2)\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2) ##### [c, h, w]!!!!!!!\n",
    "        self.act = nn.SiLU() if act else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, c1, c2, shortcut=True, k=3, e=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c2 (int): Output channels.\n",
    "            shortcut (bool): Whether to use shortcut connection.\n",
    "            g (int): Groups for convolutions.\n",
    "            k (Tuple[int, int]): Kernel sizes for convolutions.\n",
    "            e (float): Expansion ratio.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, k, 1)\n",
    "        self.cv2 = Conv(c_, c2, k, 1)\n",
    "        self.add = shortcut and c1 == c2\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply bottleneck with optional shortcut connection.\"\"\"\n",
    "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "\n",
    "\n",
    "class C2f(nn.Module):\n",
    "    def __init__(self, c1, c2, n=1, shortcut=False, e=0.5):\n",
    "        \"\"\"\n",
    "        Initialize a CSP bottleneck with 2 convolutions.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c2 (int): Output channels.\n",
    "            n (int): Number of Bottleneck blocks.\n",
    "            shortcut (bool): Whether to use shortcut connections.\n",
    "            g (int): Groups for convolutions.\n",
    "            e (float): Expansion ratio.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c1, c2, n = int(c1), int(c2), int(n)\n",
    "        self.c = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
    "        self.cv2 = Conv((2 + n) * self.c, c2, 1, 1)  # optional act=FReLU(c2)\n",
    "        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, k=3, e=1.0) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through C2f layer.\"\"\"\n",
    "        y = list(self.cv1(x).chunk(2, 1)) # B,C,H,W -> C-wise로 2등분해서 튜플로 만듬\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "        \n",
    "class SPPF(nn.Module):\n",
    "\n",
    "    def __init__(self, c1, c2, k=5):\n",
    "        \"\"\"\n",
    "        Initialize the SPPF layer with given input/output channels and kernel size.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c2 (int): Output channels.\n",
    "            k (int): Kernel size.\n",
    "\n",
    "        Notes:\n",
    "            This module is equivalent to SPP(k=(5, 9, 13)).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_ = c1 // 2  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n",
    "        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply sequential pooling operations to input and return concatenated feature maps.\"\"\"\n",
    "        y = [self.cv1(x)]\n",
    "        y.extend(self.m(y[-1]) for _ in range(3))\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Input layer\n",
    "# -----------------------------\n",
    "class Input_layer(nn.Module):\n",
    "    def __init__(self, w, det_3d_dim=5):\n",
    "        super().__init__()\n",
    "        self.detect3d = Conv(det_3d_dim, 64*w, 3, 2)\n",
    "        self.segment = Conv(1, 64*w, 3, 2)\n",
    "        self.det_3d_dim = det_3d_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if YOLOv8.mode == \"detect\":\n",
    "            if x.size()[1] != self.det_3d_dim:\n",
    "                raise Exception(f\"Expected input channel size: {self.det_3d_dim}. but you got {x.size()[1]}\")\n",
    "            x = self.detect3d(x)\n",
    "        if YOLOv8.mode == \"segment\":\n",
    "            if x.size()[1] != 1:\n",
    "                raise Exception(f\"Expected input channel size: {1}. but you got {x.size()[1]}\")\n",
    "            x = self.segment(x)\n",
    "        if YOLOv8.mode not in [\"detect\",\"segment\"]:\n",
    "             raise Exception(\"Enter the mode properly. 'detect' or 'segment'\")\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Backbone\n",
    "# -----------------------------\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, model_scale):\n",
    "        super().__init__()\n",
    "        d, w, r = model_scale[0], model_scale[1], model_scale[2]\n",
    "        self.input_layer = Input_layer(w)\n",
    "        self.stage1 = nn.Sequential(Conv(64*w, 128*w, 3, 2), C2f(128*w, 128*w, 3*d, shortcut=True)) \n",
    "        self.stage2 = nn.Sequential(Conv(128*w, 256*w, 3, 2), C2f(256*w, 256*w, 6*d, shortcut=True))\n",
    "        self.stage3 = nn.Sequential(Conv(256*w, 512*w, 3, 2), C2f(512*w, 512*w, 6*d, shortcut=True)) \n",
    "        self.stage4 = nn.Sequential(Conv(512*w, 512*w*r, 3, 2), C2f(512*w*r, 512*w*r, 3, shortcut=True), SPPF(512*w*r, 512*w*r)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x1 = self.stage1(x)\n",
    "        x2 = self.stage2(x1)\n",
    "        x3 = self.stage3(x2)\n",
    "        x4 = self.stage4(x3)\n",
    "        return [x2, x3, x4]  # C3, C4, C5 feature maps\n",
    "        \n",
    "# -----------------------------\n",
    "# Neck\n",
    "# -----------------------------\n",
    "class Neck(nn.Module):\n",
    "    def __init__(self, model_scale):\n",
    "        super().__init__()\n",
    "    \n",
    "        d, w, r = model_scale[0], model_scale[1], model_scale[2]\n",
    "    \n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.c2f1 = C2f(512*w*(1+r), 512*w, 3*d, shortcut=False)\n",
    "        \n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.c2f2 = C2f(768*w, 256*w, 3*d, shortcut=False)\n",
    "\n",
    "        self.down_conv1 = Conv(256*w, 256*w, 3, 2)\n",
    "        self.c2f3 = C2f(768*w, 512*w, 3*d, shortcut=False)\n",
    "\n",
    "        self.down_conv2 = Conv(512*w, 512*w, 3, 2)\n",
    "        self.c2f4 = C2f(512*w*(1+r), 512*w*r, 3*d, shortcut=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        c3, c4, c5 = x\n",
    "        p5 = c5\n",
    "        p4 = self.c2f1(torch.cat([self.upsample1(p5), c4], dim=1))\n",
    "        p3 = self.c2f2(torch.cat([self.upsample2(p4), c3], dim=1))\n",
    "\n",
    "        n4 = self.c2f3(torch.cat([self.down_conv1(p3), p4], dim=1))\n",
    "        n5 = self.c2f4(torch.cat([self.down_conv2(n4), p5], dim=1))\n",
    "\n",
    "        return [p3, n4, n5]\n",
    "\n",
    "# -----------------------------\n",
    "# Detect Head\n",
    "# -----------------------------\n",
    "class Detect(nn.Module):\n",
    "    \"\"\"YOLO Detect head for detection models.\"\"\"\n",
    "\n",
    "    def __init__(self, nc=1, ch=()):\n",
    "        \"\"\"Initialize the YOLO detection layer with specified number of classes and channels.\"\"\"\n",
    "        super().__init__()\n",
    "        self.nc = nc  # number of classes\n",
    "        self.nl = len(ch)  # number of detection layers\n",
    "        self.reg_max = 16  # DFL channels (ch[0] // 16 to scale 4/8/12/16/20 for n/s/m/l/x)\n",
    "        self.no = nc + self.reg_max * 4  # number of outputs per anchor\n",
    "        self.stride = torch.tensor([8, 16, 32]) \n",
    "        c2, c3 =   max((16, ch[0] // 4, self.reg_max * 4)), max(ch[0], min(self.nc, 100))  # channels\n",
    "        \n",
    "        self.cv2 = nn.ModuleList(nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch)\n",
    "        self.cv3 = (nn.ModuleList(nn.Sequential(Conv(x, c3, 3), Conv(c3, c3, 3), nn.Conv2d(c3, self.nc, 1)) for x in ch))\n",
    "        \n",
    "        self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Concatenates and returns predicted bounding boxes and class probabilities.\"\"\"\n",
    "\n",
    "        for i in range(self.nl):\n",
    "            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1) # input 채널별로 좌표, 분류 output concat\n",
    "        if YOLOv8.inference == False:\n",
    "            return x # 학습이면 그대로 출력\n",
    "        return self._inference(x)\n",
    "\n",
    "    def _inference(self, x):\n",
    "        \"\"\"\n",
    "        Decode predicted bounding boxes and class probabilities based on multiple-level feature maps.\n",
    "\n",
    "        Args:\n",
    "            x (List[torch.Tensor]): List of feature maps from different detection layers.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Concatenated tensor of decoded bounding boxes and class probabilities.\n",
    "        \"\"\"\n",
    "        # Inference path\n",
    "        shape = x[0].shape  # BCHW\n",
    "        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2) # H, nc + reg_max * 4, -1\n",
    "\n",
    "        self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n",
    "        self.shape = shape\n",
    "            \n",
    "        box, cls = x_cat.split((self.reg_max * 4, self.nc), 1) # 박스 좌표를 나타내는 feature와 분류하는 feature 분리\n",
    "\n",
    "        dbox = self.decode_bboxes(self.dfl(box), self.anchors.unsqueeze(0)) * self.strides # * self.strides: DFL로 정규화된 bbox 좌표를 원래 이미지 좌표로 변환\n",
    "\n",
    "        return torch.cat((dbox, cls.sigmoid()), 1) #classificatoin을 0~1범위로 # (B, 4+c, a)\n",
    "    \n",
    "    def decode_bboxes(self, bboxes, anchor_points, xywh=True, dim=1): # bboxes: (batch, 4, anchors),  anchor_points: (1, 2, anchors) # dim=1 \n",
    "        \"\"\"Transform bboxes(ltrb) to box(xywh or xyxy).\"\"\"\n",
    "        lt, rb = bboxes.chunk(2, dim) \n",
    "        x1y1 = anchor_points - lt # bbox의 좌측 상단(?) 좌표  # [x - l, y - t]\n",
    "        x2y2 = anchor_points + rb # bbox의 우측 하단(?) 좌표  # [x + r, y + b]\n",
    "        if xywh:\n",
    "            c_xy = (x1y1 + x2y2) / 2 # bbox의 중심\n",
    "            wh = x2y2 - x1y1 # width, height로 변환\n",
    "            return torch.cat((c_xy, wh), dim)  # xywh bbox (batch, 4, anchors)\n",
    "        return torch.cat((x1y1, x2y2), dim)  # xyxy bbox (batch, 4, anchors)\n",
    "        \n",
    "    def bias_init(self): # 매우 중요한 초기값 설정. 이를 설정할 때와 하지 않았을 때의 성능 차이가 매우 큼.\n",
    "        \"\"\"Initialize Detect() biases, WARNING: requires stride availability.\"\"\"\n",
    "        m = self  # self.model[-1]  # Detect() module\n",
    "        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1\n",
    "        # ncf = math.log(0.6 / (m.nc - 0.999999)) if cf is None else torch.log(cf / cf.sum())  # nominal class frequency\n",
    "        for a, b, s in zip(m.cv2, m.cv3, m.stride):  # from\n",
    "            a[-1].bias.data[:] = 1.0  # box\n",
    "                # box 회귀 부분의 bias를 모두 1로 초기화. 보통 중심 (cx, cy), 크기 (w, h)에 대한 초기값을 조금 조정함으로써 처음부터 박스가 전혀 예측되지 않는 현상을 피하기 위함.\n",
    "                # 예측 box의 위치가 학습 초기에 완전히 무의미하지 않도록 하고, anchor-free 구조에서 좀 더 빠른 수렴을 유도.\n",
    "            b[-1].bias.data[: m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)\n",
    "                # 확률(cls)에 대한 bias 초기값. math.log(p / (1 - p)) 형태의 로짓 초기화로, 아주 희귀한 물체가 있다는 가정. 실제로는 전체 이미지 중 약 0.01 (=1%)만이 물체를 포함한다고 가정.\n",
    "                # 학습 초기에 과도한 false positive를 줄이고, objectness + class confidence가 너무 높게 나오는 것을 막아 안정된 수렴을 유도함.\n",
    "       \n",
    "    \n",
    "# -----------------------------\n",
    "# Segment Head\n",
    "# -----------------------------\n",
    "class Proto(nn.Module):\n",
    "    \"\"\"YOLOv8 mask Proto module for segmentation models.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c_=256, c2=32): # ch[0], self.npr, self.nm\n",
    "        \"\"\"\n",
    "        Initialize the YOLOv8 mask Proto module with specified number of protos and masks.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c_ (int): Intermediate channels.\n",
    "            c2 (int): Output channels (number of protos).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cv1 = Conv(c1, c_, k=3)\n",
    "        self.upsample = nn.ConvTranspose2d(c_, c_, 2, 2, 0, bias=True)  # nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.cv2 = Conv(c_, c_, k=3)\n",
    "        self.cv3 = Conv(c_, c2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform a forward pass through layers using an upsampled input image.\"\"\"\n",
    "        return self.cv3(self.cv2(self.upsample(self.cv1(x))))\n",
    "\n",
    "class Segment(Detect):\n",
    "    \"\"\"YOLO Segment head for segmentation models.\"\"\"\n",
    "\n",
    "    def __init__(self, nc=3, nm=32, npr=256, ch=()):\n",
    "        \"\"\"Initialize the YOLO model attributes such as the number of masks, prototypes, and the convolution layers.\"\"\"\n",
    "        super().__init__(nc, ch)\n",
    "        self.nm = nm  # number of masks\n",
    "        self.npr = npr  # number of protos\n",
    "        self.proto = Proto(ch[0], self.npr, self.nm)  # protos\n",
    "        c4 = max(ch[0] // 4, self.nm)\n",
    "        self.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nm, 1)) for x in ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Return model outputs and mask coefficients if training, otherwise return outputs and mask coefficients.\"\"\"\n",
    "        p = self.proto(x[0])  # mask protos   # x[0].shape == (b, 256*w, 80, 80)\n",
    "        bs = p.shape[0]  # batch size  # P.shape == (b, nm, 160, 160)\n",
    "\n",
    "        mc = torch.cat([self.cv4[i](x[i]).view(bs, self.nm, -1) for i in range(self.nl)], 2)  # mask coefficients\n",
    "        \n",
    "        x = Detect.forward(self, x)\n",
    "        if YOLOv8.inference == False:\n",
    "            return x, mc, p\n",
    "        return (torch.cat([x, mc], 1), p) \n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# YOLOv8 Full Model\n",
    "# -----------------------------\n",
    "class YOLOv8(nn.Module):\n",
    "    mode = \"detect\"\n",
    "    inference = False\n",
    "    def __init__(self, scale=\"l\", det_num_classes=1, seg_num_classes=3):\n",
    "        super().__init__()\n",
    "        self.det_nc = det_num_classes\n",
    "        self.seg_nc = seg_num_classes\n",
    "        self.args = {\n",
    "            'box': 0.05,   # localization loss (Bbox)\n",
    "            'cls': 0.5,    # classification loss\n",
    "            'dfl': 1.5     # distribution focal loss (bounding box refinement)\n",
    "        }\n",
    "        scale = scale \n",
    "        # [depth_multiple, width_multiple, ratio]\n",
    "        model_scales = {\"n\":[0.33, 0.25, 2.0], \n",
    "                        \"s\":[0.33, 0.50, 2.0],\n",
    "                        \"m\":[0.67, 0.75, 1.5],\n",
    "                        \"l\":[1.00, 1.00, 1.0],\n",
    "                        \"x\":[1.00, 1.25, 1.0]}\n",
    "        model_scale = model_scales[scale] \n",
    "        _, w, r = model_scale[0], model_scale[1], model_scale[2]\n",
    "        \n",
    "        self.backbone = Backbone(model_scale = model_scale)\n",
    "        self.neck = Neck(model_scale = model_scale)\n",
    "        \n",
    "        self.detect = Detect(nc= det_num_classes, ch=(int(256*w), int(512*w), int(512*w*r)))\n",
    "        self.segment = Segment(nc= seg_num_classes, ch=(int(256*w), int(512*w), int(512*w*r)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.neck(x)\n",
    "        if self.mode == \"detect\" :\n",
    "            return self.detect(x)\n",
    "        elif self.mode == \"segment\":\n",
    "            return self.segment(x)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported mode. choose in 'detect', 'segment'\")\n",
    "        # return fpn_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c208c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "yolo = YOLOv8(scale=\"l\", det_num_classes=1, seg_num_classes=3).to(device)\n",
    "yolo.detect.bias_init()\n",
    "yolo.segment.bias_init()\n",
    "YOLOv8.mode = \"detect\"\n",
    "\n",
    "state_dict = torch.load(\"/kaggle/input/multi-head-trained-yolov8/pytorch/default/2/4epoch_best_multi_head_trained_yolo.pt\", weights_only=True)\n",
    "yolo.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c362eb",
   "metadata": {},
   "source": [
    "**Z를 예측하는 ResNetMLP 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d615e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetMLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_classes=11, \n",
    "                 in_chans=1, \n",
    "                 embed_dim = 512,\n",
    "                 num_patch=5,\n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. ResNet18 feature extractor\n",
    "        backbone = models.resnet18(weights=None)\n",
    "        backbone.conv1 = nn.Conv2d(in_channels=in_chans, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        self.patch_encoder = create_feature_extractor( # backbone의 중간 출력을 추출하는 기능\n",
    "            backbone,\n",
    "            return_nodes={\"avgpool\": \"feat\"} # dict -> key :\"feat\", value : avgpool layer output\n",
    "        )\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # 6. Classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(num_patch * self.embed_dim, num_patch * self.embed_dim),\n",
    "            nn.BatchNorm1d(num_patch * self.embed_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(num_patch * self.embed_dim, num_patch * self.embed_dim),\n",
    "            nn.BatchNorm1d(num_patch * self.embed_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(num_patch * self.embed_dim, num_patch * self.embed_dim),\n",
    "            nn.BatchNorm1d(num_patch * self.embed_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(num_patch * self.embed_dim, num_classes)  # hidden_layer -> output_layer\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, N, H, W) — B=batch, N=patches, H,W=patch size\n",
    "        \"\"\"\n",
    "        B, N, H, W = x.shape\n",
    "\n",
    "        # Flatten into B*N fake-batch\n",
    "        x = x.view(-1, 1, H, W)  \n",
    "\n",
    "        feats = self.patch_encoder(x)[\"feat\"]  # (B*N, 512, 1, 1)\n",
    "        feats = feats.view(B, N*self.embed_dim)  # (B, N*D)\n",
    "        \n",
    "        logits = self.head(feats)  # (B, num_classes)\n",
    "\n",
    "        return logits\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "resnet_mlp = ResNetMLP(\n",
    "    num_classes=11,\n",
    "    in_chans=1).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d262ae0",
   "metadata": {},
   "source": [
    "### Loss function 구현\n",
    "\n",
    "**Detect loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "def bbox_iou(box1, box2, xywh=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) between bounding boxes.\n",
    "\n",
    "    This function supports various shapes for `box1` and `box2` as long as the last dimension is 4.\n",
    "    For instance, you may pass tensors shaped like (4,), (N, 4), (B, N, 4), or (B, N, 1, 4).\n",
    "    Internally, the code will split the last dimension into (x, y, w, h) if `xywh=True`,\n",
    "    or (x1, y1, x2, y2) if `xywh=False`.\n",
    "\n",
    "    Args:\n",
    "        box1 (torch.Tensor): A tensor representing one or more bounding boxes, with the last dimension being 4.\n",
    "        box2 (torch.Tensor): A tensor representing one or more bounding boxes, with the last dimension being 4.\n",
    "        xywh (bool, optional): If True, input boxes are in (x, y, w, h) format. If False, input boxes are in\n",
    "                               (x1, y1, x2, y2) format.\n",
    "        GIoU (bool, optional): If True, calculate Generalized IoU.\n",
    "        DIoU (bool, optional): If True, calculate Distance IoU.\n",
    "        CIoU (bool, optional): If True, calculate Complete IoU.\n",
    "        eps (float, optional): A small value to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): IoU, GIoU, DIoU, or CIoU values depending on the specified flags.\n",
    "    \"\"\"\n",
    "    # Get the coordinates of bounding boxes\n",
    "    if xywh:  # transform from xywh to xyxy\n",
    "        (x1, y1, w1, h1), (x2, y2, w2, h2) = box1.chunk(4, -1), box2.chunk(4, -1)\n",
    "        w1_, h1_, w2_, h2_ = w1 / 2, h1 / 2, w2 / 2, h2 / 2\n",
    "        b1_x1, b1_x2, b1_y1, b1_y2 = x1 - w1_, x1 + w1_, y1 - h1_, y1 + h1_\n",
    "        b2_x1, b2_x2, b2_y1, b2_y2 = x2 - w2_, x2 + w2_, y2 - h2_, y2 + h2_\n",
    "    else:  # x1, y1, x2, y2 = box1\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
    "        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
    "        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
    "\n",
    "    # Intersection area\n",
    "    inter = (b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)).clamp_(0) * (\n",
    "        b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)\n",
    "    ).clamp_(0)\n",
    "\n",
    "    # Union Area\n",
    "    union = w1 * h1 + w2 * h2 - inter + eps\n",
    "\n",
    "    # IoU\n",
    "    iou = inter / union\n",
    "    if CIoU or DIoU or GIoU:\n",
    "        cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex (smallest enclosing box) width\n",
    "        ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\n",
    "        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n",
    "            c2 = cw.pow(2) + ch.pow(2) + eps  # convex diagonal squared\n",
    "            rho2 = (\n",
    "                (b2_x1 + b2_x2 - b1_x1 - b1_x2).pow(2) + (b2_y1 + b2_y2 - b1_y1 - b1_y2).pow(2)\n",
    "            ) / 4  # center dist**2\n",
    "            if CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "                v = (4 / math.pi**2) * ((w2 / h2).atan() - (w1 / h1).atan()).pow(2)\n",
    "                with torch.no_grad():\n",
    "                    alpha = v / (v - iou + (1 + eps))\n",
    "                return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
    "            return iou - rho2 / c2  # DIoU\n",
    "        c_area = cw * ch + eps  # convex area\n",
    "        return iou - (c_area - union) / c_area  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n",
    "    return iou  # IoU\n",
    "\n",
    "\n",
    "class TaskAlignedAssigner(nn.Module):\n",
    "    \"\"\"\n",
    "    A task-aligned assigner for object detection.\n",
    "\n",
    "    This class assigns ground-truth (gt) objects to anchors based on the task-aligned metric, which combines both\n",
    "    classification and localization information.\n",
    "\n",
    "    Attributes:\n",
    "        topk (int): The number of top candidates to consider.\n",
    "        num_classes (int): The number of object classes.\n",
    "        bg_idx (int): Background class index.\n",
    "        alpha (float): The alpha parameter for the classification component of the task-aligned metric.\n",
    "        beta (float): The beta parameter for the localization component of the task-aligned metric.\n",
    "        eps (float): A small value to prevent division by zero.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, topk=13, num_classes=80, alpha=1.0, beta=6.0, eps=1e-9): #(topk=10, num_classes=self.nc, alpha=0.5, beta=6.0)\n",
    "\n",
    "        \"\"\"Initialize a TaskAlignedAssigner object with customizable hyperparameters.\"\"\"\n",
    "        super().__init__()\n",
    "        self.topk = topk\n",
    "        self.num_classes = num_classes\n",
    "        self.bg_idx = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n",
    "            # pred_scores.detach().sigmoid(), # (batch, anchors, c)\n",
    "            # (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype), # (b, h*w, 4)\n",
    "            # anchor_points * stride_tensor, # (2, anchors)\n",
    "            # gt_labels, # (B, counts.max, 1)\n",
    "            # gt_bboxes, # (B, counts.max, 4)\n",
    "            # mask_gt, # (B, counts.max, 1)\n",
    "        \"\"\"\n",
    "        Compute the task-aligned assignment.\n",
    "\n",
    "        Args:\n",
    "            pd_scores (torch.Tensor): Predicted classification scores with shape (bs, num_total_anchors, num_classes).\n",
    "            pd_bboxes (torch.Tensor): Predicted bounding boxes with shape (bs, num_total_anchors, 4).\n",
    "            anc_points (torch.Tensor): Anchor points with shape (num_total_anchors, 2).\n",
    "            gt_labels (torch.Tensor): Ground truth labels with shape (bs, n_max_boxes, 1).\n",
    "            gt_bboxes (torch.Tensor): Ground truth boxes with shape (bs, n_max_boxes, 4).\n",
    "            mask_gt (torch.Tensor): Mask for valid ground truth boxes with shape (bs, n_max_boxes, 1).\n",
    "\n",
    "        Returns:\n",
    "            target_labels (torch.Tensor): Target labels with shape (bs, num_total_anchors).\n",
    "            target_bboxes (torch.Tensor): Target bounding boxes with shape (bs, num_total_anchors, 4).\n",
    "            target_scores (torch.Tensor): Target scores with shape (bs, num_total_anchors, num_classes).\n",
    "            fg_mask (torch.Tensor): Foreground mask with shape (bs, num_total_anchors).\n",
    "            target_gt_idx (torch.Tensor): Target ground truth indices with shape (bs, num_total_anchors).\n",
    "\n",
    "        References:\n",
    "            https://github.com/Nioolek/PPYOLOE_pytorch/blob/master/ppyoloe/assigner/tal_assigner.py\n",
    "        \"\"\"\n",
    "        self.bs = pd_scores.shape[0] # batch_size\n",
    "        self.n_max_boxes = gt_bboxes.shape[1] \n",
    "        device = gt_bboxes.device\n",
    "\n",
    "        if self.n_max_boxes == 0:\n",
    "            return (\n",
    "                torch.full_like(pd_scores[..., 0], self.bg_idx),\n",
    "                torch.zeros_like(pd_bboxes),\n",
    "                torch.zeros_like(pd_scores),\n",
    "                torch.zeros_like(pd_scores[..., 0]),\n",
    "                torch.zeros_like(pd_scores[..., 0]),\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            return self._forward(pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            # Move tensors to CPU, compute, then move back to original device\n",
    "            LOGGER.warning(\"WARNING: CUDA OutOfMemoryError in TaskAlignedAssigner, using CPU\")\n",
    "            cpu_tensors = [t.cpu() for t in (pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)]\n",
    "            result = self._forward(*cpu_tensors)\n",
    "            return tuple(t.to(device) for t in result)\n",
    "\n",
    "    def _forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n",
    "        \"\"\"\n",
    "        Compute the task-aligned assignment.\n",
    "\n",
    "        Args:\n",
    "            pd_scores (torch.Tensor): Predicted classification scores with shape (bs, num_total_anchors, num_classes).\n",
    "            pd_bboxes (torch.Tensor): Predicted bounding boxes with shape (bs, num_total_anchors, 4).\n",
    "            anc_points (torch.Tensor): Anchor points with shape (num_total_anchors, 2).\n",
    "            gt_labels (torch.Tensor): Ground truth labels with shape (bs, n_max_boxes, 1).\n",
    "            gt_bboxes (torch.Tensor): Ground truth boxes with shape (bs, n_max_boxes, 4).\n",
    "            mask_gt (torch.Tensor): Mask for valid ground truth boxes with shape (bs, n_max_boxes, 1).\n",
    "\n",
    "        Returns:\n",
    "            target_labels (torch.Tensor): Target labels with shape (bs, num_total_anchors).\n",
    "            target_bboxes (torch.Tensor): Target bounding boxes with shape (bs, num_total_anchors, 4).\n",
    "            target_scores (torch.Tensor): Target scores with shape (bs, num_total_anchors, num_classes).\n",
    "            fg_mask (torch.Tensor): Foreground mask with shape (bs, num_total_anchors).\n",
    "            target_gt_idx (torch.Tensor): Target ground truth indices with shape (bs, num_total_anchors).\n",
    "        \"\"\"\n",
    "        mask_pos, align_metric, overlaps = self.get_pos_mask(\n",
    "            pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt\n",
    "        )\n",
    "\n",
    "        target_gt_idx, fg_mask, mask_pos = self.select_highest_overlaps(mask_pos, overlaps, self.n_max_boxes)\n",
    "\n",
    "        # Assigned target\n",
    "        target_labels, target_bboxes, target_scores = self.get_targets(gt_labels, gt_bboxes, target_gt_idx, fg_mask)\n",
    "\n",
    "        # Normalize\n",
    "        align_metric *= mask_pos\n",
    "        pos_align_metrics = align_metric.amax(dim=-1, keepdim=True)  # b, max_num_obj\n",
    "        pos_overlaps = (overlaps * mask_pos).amax(dim=-1, keepdim=True)  # b, max_num_obj\n",
    "        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2).unsqueeze(-1)\n",
    "        target_scores = target_scores * norm_align_metric\n",
    "\n",
    "        return target_labels, target_bboxes, target_scores, fg_mask.bool(), target_gt_idx\n",
    "\n",
    "    def get_pos_mask(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt):\n",
    "        \"\"\"\n",
    "        Get positive mask for each ground truth box.\n",
    "\n",
    "        Args:\n",
    "            pd_scores (torch.Tensor): Predicted classification scores with shape (bs, num_total_anchors, num_classes).\n",
    "            pd_bboxes (torch.Tensor): Predicted bounding boxes with shape (bs, num_total_anchors, 4).\n",
    "            gt_labels (torch.Tensor): Ground truth labels with shape (bs, n_max_boxes, 1).\n",
    "            gt_bboxes (torch.Tensor): Ground truth boxes with shape (bs, n_max_boxes, 4).\n",
    "            anc_points (torch.Tensor): Anchor points with shape (num_total_anchors, 2).\n",
    "            mask_gt (torch.Tensor): Mask for valid ground truth boxes with shape (bs, n_max_boxes, 1).\n",
    "\n",
    "        Returns:\n",
    "            mask_pos (torch.Tensor): Positive mask with shape (bs, max_num_obj, h*w).\n",
    "            align_metric (torch.Tensor): Alignment metric with shape (bs, max_num_obj, h*w).\n",
    "            overlaps (torch.Tensor): Overlaps between predicted and ground truth boxes with shape (bs, max_num_obj, h*w).\n",
    "        \"\"\"\n",
    "        mask_in_gts = self.select_candidates_in_gts(anc_points, gt_bboxes)  # (B, num_max_boxes, anchors)\n",
    "        # Get anchor_align metric, (b, max_num_obj, h*w)\n",
    "        align_metric, overlaps = self.get_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_in_gts * mask_gt)\n",
    "        # Get topk_metric mask, (b, max_num_obj, h*w)\n",
    "        mask_topk = self.select_topk_candidates(align_metric, topk_mask=mask_gt.expand(-1, -1, self.topk).bool())\n",
    "        # Merge all mask to a final mask, (b, max_num_obj, h*w)\n",
    "        mask_pos = mask_topk * mask_in_gts * mask_gt\n",
    "\n",
    "        return mask_pos, align_metric, overlaps\n",
    "\n",
    "    def get_box_metrics(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_gt): # (pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_in_gts * mask_gt)\n",
    "        \"\"\"\n",
    "        Compute alignment metric given predicted and ground truth bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            pd_scores (torch.Tensor): Predicted classification scores with shape (bs, num_total_anchors, num_classes).\n",
    "            pd_bboxes (torch.Tensor): Predicted bounding boxes with shape (bs, num_total_anchors, 4).\n",
    "            gt_labels (torch.Tensor): Ground truth labels with shape (bs, n_max_boxes, 1).\n",
    "            gt_bboxes (torch.Tensor): Ground truth boxes with shape (bs, n_max_boxes, 4).\n",
    "            mask_gt (torch.Tensor): Mask for valid ground truth boxes with shape (bs, n_max_boxes, h*w).\n",
    "\n",
    "        Returns:\n",
    "            align_metric (torch.Tensor): Alignment metric combining classification and localization.\n",
    "            overlaps (torch.Tensor): IoU overlaps between predicted and ground truth boxes.\n",
    "        \"\"\"\n",
    "        na = pd_bboxes.shape[-2] # num_total_anchors\n",
    "        mask_gt = mask_gt.bool()  # mask_gt = mask_in_gts * mask_gt -> (b, max_max_boxes, a) # 1과 0으로 이뤄진 mask를 boolean으로 변환\n",
    "        overlaps = torch.zeros([self.bs, self.n_max_boxes, na], dtype=pd_bboxes.dtype, device=pd_bboxes.device) # (b, nmb, a)\n",
    "        bbox_scores = torch.zeros([self.bs, self.n_max_boxes, na], dtype=pd_scores.dtype, device=pd_scores.device) # (b, nmb, a)\n",
    "\n",
    "        ind = torch.zeros([2, self.bs, self.n_max_boxes], dtype=torch.long)  # (2, b, nmb)\n",
    "        ind[0] = torch.arange(end=self.bs).view(-1, 1).expand(-1, self.n_max_boxes)  # (b, nmb)\n",
    "        ind[1] = gt_labels.squeeze(-1)  # (b, nmb)\n",
    "        # Get the scores of each grid for each gt cls\n",
    "        bbox_scores[mask_gt] = pd_scores[ind[0], :, ind[1]][mask_gt]  # b, max_num_obj, h*w\n",
    "\n",
    "        # (b, max_num_obj, 1, 4), (b, 1, h*w, 4)\n",
    "        pd_boxes = pd_bboxes.unsqueeze(1).expand(-1, self.n_max_boxes, -1, -1)[mask_gt]\n",
    "        gt_boxes = gt_bboxes.unsqueeze(2).expand(-1, -1, na, -1)[mask_gt]\n",
    "        overlaps[mask_gt] = self.iou_calculation(gt_boxes, pd_boxes)\n",
    "\n",
    "        align_metric = bbox_scores.pow(self.alpha) * overlaps.pow(self.beta)\n",
    "        return align_metric, overlaps\n",
    "\n",
    "    def iou_calculation(self, gt_bboxes, pd_bboxes):\n",
    "        \"\"\"\n",
    "        Calculate IoU for horizontal bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            gt_bboxes (torch.Tensor): Ground truth boxes.\n",
    "            pd_bboxes (torch.Tensor): Predicted boxes.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): IoU values between each pair of boxes.\n",
    "        \"\"\"\n",
    "        return bbox_iou(gt_bboxes, pd_bboxes, xywh=False, CIoU=True).squeeze(-1).clamp_(0)\n",
    "\n",
    "    def select_topk_candidates(self, metrics, largest=True, topk_mask=None):\n",
    "        \"\"\"\n",
    "        Select the top-k candidates based on the given metrics.\n",
    "\n",
    "        Args:\n",
    "            metrics (torch.Tensor): A tensor of shape (b, max_num_obj, h*w), where b is the batch size,\n",
    "                              max_num_obj is the maximum number of objects, and h*w represents the\n",
    "                              total number of anchor points.\n",
    "            largest (bool): If True, select the largest values; otherwise, select the smallest values.\n",
    "            topk_mask (torch.Tensor): An optional boolean tensor of shape (b, max_num_obj, topk), where\n",
    "                                topk is the number of top candidates to consider. If not provided,\n",
    "                                the top-k values are automatically computed based on the given metrics.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): A tensor of shape (b, max_num_obj, h*w) containing the selected top-k candidates.\n",
    "        \"\"\"\n",
    "        # (b, max_num_obj, topk)\n",
    "        topk_metrics, topk_idxs = torch.topk(metrics, self.topk, dim=-1, largest=largest)\n",
    "        if topk_mask is None:\n",
    "            topk_mask = (topk_metrics.max(-1, keepdim=True)[0] > self.eps).expand_as(topk_idxs)\n",
    "        # (b, max_num_obj, topk)\n",
    "        topk_idxs.masked_fill_(~topk_mask, 0)\n",
    "\n",
    "        # (b, max_num_obj, topk, h*w) -> (b, max_num_obj, h*w)\n",
    "        count_tensor = torch.zeros(metrics.shape, dtype=torch.int8, device=topk_idxs.device)\n",
    "        ones = torch.ones_like(topk_idxs[:, :, :1], dtype=torch.int8, device=topk_idxs.device)\n",
    "        for k in range(self.topk):\n",
    "            # Expand topk_idxs for each value of k and add 1 at the specified positions\n",
    "            count_tensor.scatter_add_(-1, topk_idxs[:, :, k : k + 1], ones)\n",
    "        # Filter invalid bboxes\n",
    "        count_tensor.masked_fill_(count_tensor > 1, 0)\n",
    "\n",
    "        return count_tensor.to(metrics.dtype)\n",
    "\n",
    "    def get_targets(self, gt_labels, gt_bboxes, target_gt_idx, fg_mask):\n",
    "        \"\"\"\n",
    "        Compute target labels, target bounding boxes, and target scores for the positive anchor points.\n",
    "\n",
    "        Args:\n",
    "            gt_labels (torch.Tensor): Ground truth labels of shape (b, max_num_obj, 1), where b is the\n",
    "                                batch size and max_num_obj is the maximum number of objects.\n",
    "            gt_bboxes (torch.Tensor): Ground truth bounding boxes of shape (b, max_num_obj, 4).\n",
    "            target_gt_idx (torch.Tensor): Indices of the assigned ground truth objects for positive\n",
    "                                    anchor points, with shape (b, h*w), where h*w is the total\n",
    "                                    number of anchor points.\n",
    "            fg_mask (torch.Tensor): A boolean tensor of shape (b, h*w) indicating the positive\n",
    "                              (foreground) anchor points.\n",
    "\n",
    "        Returns:\n",
    "            target_labels (torch.Tensor): Shape (b, h*w), containing the target labels for positive anchor points.\n",
    "            target_bboxes (torch.Tensor): Shape (b, h*w, 4), containing the target bounding boxes for positive\n",
    "                                          anchor points.\n",
    "            target_scores (torch.Tensor): Shape (b, h*w, num_classes), containing the target scores for positive\n",
    "                                          anchor points.\n",
    "        \"\"\"\n",
    "        # Assigned target labels, (b, 1)\n",
    "        batch_ind = torch.arange(end=self.bs, dtype=torch.int64, device=gt_labels.device)[..., None]\n",
    "        target_gt_idx = target_gt_idx + batch_ind * self.n_max_boxes  # (b, h*w)\n",
    "        target_labels = gt_labels.long().flatten()[target_gt_idx]  # (b, h*w)\n",
    "\n",
    "        # Assigned target boxes, (b, max_num_obj, 4) -> (b, h*w, 4)\n",
    "        target_bboxes = gt_bboxes.view(-1, gt_bboxes.shape[-1])[target_gt_idx]\n",
    "\n",
    "        # Assigned target scores\n",
    "        target_labels.clamp_(0)\n",
    "\n",
    "        # 10x faster than F.one_hot()\n",
    "        target_scores = torch.zeros(\n",
    "            (target_labels.shape[0], target_labels.shape[1], self.num_classes),\n",
    "            dtype=torch.int64,\n",
    "            device=target_labels.device,\n",
    "        )  # (b, h*w, 80)\n",
    "        target_scores.scatter_(2, target_labels.unsqueeze(-1), 1)\n",
    "\n",
    "        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.num_classes)  # (b, h*w, 80)\n",
    "        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n",
    "\n",
    "        return target_labels, target_bboxes, target_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def select_candidates_in_gts(xy_centers, gt_bboxes, eps=1e-9): # (anc_points, gt_bboxes)\n",
    "        \"\"\"\n",
    "        Select positive anchor centers within ground truth bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            xy_centers (torch.Tensor): Anchor center coordinates, shape (h*w, 2).\n",
    "            gt_bboxes (torch.Tensor): Ground truth bounding boxes, shape (b, n_boxes, 4).\n",
    "            eps (float, optional): Small value for numerical stability. Defaults to 1e-9.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Boolean mask of positive anchors, shape (b, n_boxes, h*w).\n",
    "\n",
    "        Note:\n",
    "            b: batch size, n_boxes: number of ground truth boxes, h: height, w: width.\n",
    "            Bounding box format: [x_min, y_min, x_max, y_max].\n",
    "        \"\"\"\n",
    "        n_anchors = xy_centers.shape[0] # num_anchors\n",
    "        bs, n_boxes, _ = gt_bboxes.shape # n_boxes = num_max_boxes\n",
    "        lt, rb = gt_bboxes.view(-1, 1, 4).chunk(2, 2)  # left-top, right-bottom\n",
    "        bbox_deltas = torch.cat((xy_centers[None] - lt, rb - xy_centers[None]), dim=2).view(bs, n_boxes, n_anchors, -1) # (bs, n_boxes, a, 4)\n",
    "            # x[None]: 맨 앞에 차원 추가 \n",
    "            # xy_centers[None] -> (1, a, 2) // lt or rb -> (bs*n_boxes, 1, 2)  # xy_centers[None] - lt -> (bs*n_boxes, a, 2) -# cat -> (bs*n_boxes, a, 4)\n",
    "        return bbox_deltas.amin(3).gt_(eps) # (bs, n_boxes, a)\n",
    "\n",
    "    @staticmethod\n",
    "    def select_highest_overlaps(mask_pos, overlaps, n_max_boxes):\n",
    "        \"\"\"\n",
    "        Select anchor boxes with highest IoU when assigned to multiple ground truths.\n",
    "\n",
    "        Args:\n",
    "            mask_pos (torch.Tensor): Positive mask, shape (b, n_max_boxes, h*w).\n",
    "            overlaps (torch.Tensor): IoU overlaps, shape (b, n_max_boxes, h*w).\n",
    "            n_max_boxes (int): Maximum number of ground truth boxes.\n",
    "\n",
    "        Returns:\n",
    "            target_gt_idx (torch.Tensor): Indices of assigned ground truths, shape (b, h*w).\n",
    "            fg_mask (torch.Tensor): Foreground mask, shape (b, h*w).\n",
    "            mask_pos (torch.Tensor): Updated positive mask, shape (b, n_max_boxes, h*w).\n",
    "        \"\"\"\n",
    "        # Convert (b, n_max_boxes, h*w) -> (b, h*w)\n",
    "        fg_mask = mask_pos.sum(-2)\n",
    "        if fg_mask.max() > 1:  # one anchor is assigned to multiple gt_bboxes\n",
    "            mask_multi_gts = (fg_mask.unsqueeze(1) > 1).expand(-1, n_max_boxes, -1)  # (b, n_max_boxes, h*w)\n",
    "            max_overlaps_idx = overlaps.argmax(1)  # (b, h*w)\n",
    "\n",
    "            is_max_overlaps = torch.zeros(mask_pos.shape, dtype=mask_pos.dtype, device=mask_pos.device)\n",
    "            is_max_overlaps.scatter_(1, max_overlaps_idx.unsqueeze(1), 1)\n",
    "\n",
    "            mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos).float()  # (b, n_max_boxes, h*w)\n",
    "            fg_mask = mask_pos.sum(-2)\n",
    "        # Find each grid serve which gt(index)\n",
    "        target_gt_idx = mask_pos.argmax(-2)  # (b, h*w)\n",
    "        return target_gt_idx, fg_mask, mask_pos\n",
    "\n",
    "class DFLoss(nn.Module):\n",
    "    \"\"\"Criterion class for computing Distribution Focal Loss (DFL).\"\"\"\n",
    "\n",
    "    def __init__(self, reg_max=16) -> None:\n",
    "        \"\"\"Initialize the DFL module with regularization maximum.\"\"\"\n",
    "        super().__init__()\n",
    "        self.reg_max = reg_max\n",
    "\n",
    "    def __call__(self, pred_dist, target):\n",
    "        \"\"\"Return sum of left and right DFL losses from https://ieeexplore.ieee.org/document/9792391.\"\"\"\n",
    "        target = target.clamp_(0, self.reg_max - 1 - 0.01)\n",
    "        tl = target.long()  # target left\n",
    "        tr = tl + 1  # target right\n",
    "        wl = tr - target  # weight left\n",
    "        wr = 1 - wl  # weight right\n",
    "        return (\n",
    "            F.cross_entropy(pred_dist, tl.view(-1), reduction=\"none\").view(tl.shape) * wl\n",
    "            + F.cross_entropy(pred_dist, tr.view(-1), reduction=\"none\").view(tl.shape) * wr\n",
    "        ).mean(-1, keepdim=True)\n",
    "\n",
    "class BboxLoss(nn.Module):\n",
    "    \"\"\"Criterion class for computing training losses for bounding boxes.\"\"\"\n",
    "\n",
    "    def __init__(self, reg_max=16):\n",
    "        \"\"\"Initialize the BboxLoss module with regularization maximum and DFL settings.\"\"\"\n",
    "        super().__init__()\n",
    "        self.dfl_loss = DFLoss(reg_max) if reg_max > 1 else None\n",
    "\n",
    "    def forward(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask):\n",
    "        \"\"\"Compute IoU and DFL losses for bounding boxes.\"\"\"\n",
    "        weight = target_scores.sum(-1)[fg_mask].unsqueeze(-1)\n",
    "        iou = bbox_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask], xywh=False, CIoU=True)\n",
    "        loss_iou = ((1.0 - iou) * weight).sum() / target_scores_sum\n",
    "\n",
    "        # DFL loss\n",
    "        if self.dfl_loss:\n",
    "            target_ltrb = bbox2dist(anchor_points, target_bboxes, self.dfl_loss.reg_max - 1)\n",
    "            loss_dfl = self.dfl_loss(pred_dist[fg_mask].view(-1, self.dfl_loss.reg_max), target_ltrb[fg_mask]) * weight\n",
    "            loss_dfl = loss_dfl.sum() / target_scores_sum\n",
    "        else:\n",
    "            loss_dfl = torch.tensor(0.0).to(pred_dist.device)\n",
    "\n",
    "        return loss_iou, loss_dfl\n",
    "\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n",
    "    top-left corner and (x2, y2) is the bottom-right corner. Note: ops per 2 channels faster than per channel.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "    \"\"\"\n",
    "    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
    "    y = torch.empty_like(x)  # faster than clone/copy\n",
    "    xy = x[..., :2]  # centers\n",
    "    wh = x[..., 2:] / 2  # half width-height\n",
    "    y[..., :2] = xy - wh  # top left xy\n",
    "    y[..., 2:] = xy + wh  # bottom right xy\n",
    "    return y\n",
    "\n",
    "def dist2bbox(distance, anchor_points, xywh=True, dim=-1): # distance: (batch, 4, anchors),  anchor_points: (1, 2, anchors) # dim=1 \n",
    "    \"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\n",
    "    lt, rb = distance.chunk(2, dim) \n",
    "    x1y1 = anchor_points - lt # bbox의 좌측 상단(?) 좌표  # [x - l, y - t]\n",
    "    x2y2 = anchor_points + rb # bbox의 우측 하단(?) 좌표  # [x + r, y + b]\n",
    "    if xywh:\n",
    "        c_xy = (x1y1 + x2y2) / 2 # bbox의 중심\n",
    "        wh = x2y2 - x1y1 # width, height로 변환\n",
    "        return torch.cat((c_xy, wh), dim)  # xywh bbox (batch, 4, anchors)\n",
    "    return torch.cat((x1y1, x2y2), dim)  # xyxy bbox (batch, 4, anchors)\n",
    "\n",
    "def bbox2dist(anchor_points, bbox, reg_max):\n",
    "    \"\"\"Transform bbox(xyxy) to dist(ltrb).\"\"\"\n",
    "    x1y1, x2y2 = bbox.chunk(2, -1)\n",
    "    return torch.cat((anchor_points - x1y1, x2y2 - anchor_points), -1).clamp_(0, reg_max - 0.01)  # dist (lt, rb)\n",
    "\n",
    "class v8DetectionLoss:\n",
    "    \"\"\"Criterion class for computing training losses for YOLOv8 object detection.\"\"\"\n",
    "\n",
    "    def __init__(self, args, model_head, tal_topk=10):  # model must be de-paralleled\n",
    "        \"\"\"Initialize v8DetectionLoss with model parameters and task-aligned assignment settings.\"\"\"\n",
    "        device = next(model_head.parameters()).device  # get model device\n",
    "        h = args  # hyperparameters\n",
    "        m = model_head  # Detect() module\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.hyp = h\n",
    "        self.stride = m.stride  # model strides\n",
    "        self.nc = m.nc  # number of classes\n",
    "        self.no = m.nc + m.reg_max * 4\n",
    "        self.reg_max = m.reg_max\n",
    "        self.device = device\n",
    "\n",
    "        self.use_dfl = m.reg_max > 1\n",
    "\n",
    "        self.assigner = TaskAlignedAssigner(topk=tal_topk, num_classes=self.nc, alpha=0.5, beta=6.0)\n",
    "        self.bbox_loss = BboxLoss(m.reg_max).to(device)\n",
    "        self.proj = torch.arange(m.reg_max, dtype=torch.float, device=device) # [0,1,...,16]\n",
    "\n",
    "    def preprocess(self, targets, batch_size, scale_tensor):\n",
    "        \"\"\"Preprocess targets by converting to tensor format and scaling coordinates.\"\"\"\n",
    "        nl, ne = targets.shape # nl=num_GT, ne=1+1+4\n",
    "        if nl == 0:\n",
    "            out = torch.zeros(batch_size, 0, ne - 1, device=self.device) # (b, 0, 5) -> 차원에 0이 있으면 빈 텐서 반환\n",
    "        else:\n",
    "            i = targets[:, 0]  # image index \n",
    "            _, counts = i.unique(return_counts=True) # 유일값과 그 유일값이 원래 몇개있었는지를 함께 반환 \n",
    "                # torch.tensor([1,1,2,2,3,4]).unique(return_counts=True) -> (tensor([1, 2, 3, 4]), tensor([2, 2, 1, 1]))\n",
    "            counts = counts.to(dtype=torch.int32)\n",
    "            out = torch.zeros(batch_size, counts.max(), ne - 1, device=self.device) # (B, counts.max, 5) # counts.max: 한 이미지가 가진 GT 개수 중 가장 많은 GT 개\n",
    "            for j in range(batch_size): # image idx는 batch 기준으로 0~batch_zie로 지정하는 것으로 보임\n",
    "                matches = i == j  # 텐서 i의 원소와 스칼라 j를 비교하여 boolean 텐서 반환 # j번째 이미지가 가지고 있는 GT들을 볼 수 있는 mask\n",
    "                if n := matches.sum(): # := -> 조건문에서 변수에 할당할 수 있는 월러스 연산자. # GT가 없으면 0임으로 False 하나 이상 있으면 True\n",
    "                    out[j, :n] = targets[matches, 1:] # n: j번째 이미지가 가지고 있는 GT 수 # traget에서 j번째 이미지에 해당하는 cls와 boxes를 가져온다.\n",
    "            out[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor)) # out[..., 1:5]->(B, counts.max, 4) /// scale_tensor->(4)\n",
    "        return out\n",
    "\n",
    "    def bbox_decode(self, anchor_points, pred_dist):\n",
    "        \"\"\"Decode predicted object bounding box coordinates from anchor points and distribution.\"\"\"\n",
    "        if self.use_dfl:\n",
    "            b, a, c = pred_dist.shape  # batch, anchors, channels\n",
    "            pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype)) # (b, a, 4, 16) -> (b, a, 4, 0) -> (b, a, 4)\n",
    "            # pred_dist = pred_dist.view(b, a, c // 4, 4).transpose(2,3).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
    "            # pred_dist = (pred_dist.view(b, a, c // 4, 4).softmax(2) * self.proj.type(pred_dist.dtype).view(1, 1, -1, 1)).sum(2)\n",
    "        return dist2bbox(pred_dist, anchor_points, xywh=False) # (b, a, 4)\n",
    "\n",
    "    def __call__(self, preds, batch):\n",
    "        \"\"\"Calculate the sum of the loss for box, cls and dfl multiplied by batch size.\"\"\"\n",
    "        loss = torch.zeros(3, device=self.device)  # box, cls, dfl\n",
    "        feats = preds[1] if isinstance(preds, tuple) else preds # 리스트임으로 preds\n",
    "        pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(   # feats[0].shape[0]==batch\n",
    "            (self.reg_max * 4, self.nc), 1\n",
    "        )\n",
    "\n",
    "        pred_scores = pred_scores.permute(0, 2, 1).contiguous() # (batch, c, anchors) -> (batch, anchors, c)\n",
    "        pred_distri = pred_distri.permute(0, 2, 1).contiguous() # (batch, 16*4, anchors) -> (batch, anchors, 16*4)\n",
    "\n",
    "        dtype = pred_scores.dtype\n",
    "        batch_size = pred_scores.shape[0]\n",
    "        imgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\n",
    "        anchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5) # (2, anchors(8400))\n",
    "\n",
    "        # Targets\n",
    "        targets = torch.cat((batch[\"batch_idx\"].view(-1, 1), batch[\"cls\"].view(-1, 1), batch[\"bboxes\"]), 1) # -> (num_GT, 1+1+4)\n",
    "            # batch = {\"batch_idx\": tensor([0, 0, 1]),  # 3개의 GT가 있고, 2개는 이미지 0에, 1개는 이미지 1에 속함\n",
    "            #          \"cls\": tensor([2, 5, 1]),        # 각각 클래스 2, 5, 1\n",
    "            #          \"bboxes\": tensor([\n",
    "            #             [0.1, 0.1, 0.2, 0.2],\n",
    "            #             [0.3, 0.3, 0.5, 0.5],\n",
    "            #             [0.4, 0.4, 0.6, 0.6]])}\n",
    "        targets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]]) # imgsz[[1, 0, 1, 0]] = (w,h,w,h) # (B, counts.max, 1+4)\n",
    "        gt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\n",
    "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0.0) # (B, counts.max, 4) -> (B, counts.max, 1) # x.gt_(y) : x > y면 1 아니면 0 # GT가 있으면 1아니면 0\n",
    "\n",
    "        # Pboxes\n",
    "        pred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n",
    "        # dfl_conf = pred_distri.view(batch_size, -1, 4, self.reg_max).detach().softmax(-1)\n",
    "        # dfl_conf = (dfl_conf.amax(-1).mean(-1) + dfl_conf.amax(-1).amin(-1)) / 2\n",
    "\n",
    "        _, target_bboxes, target_scores, fg_mask, _ = self.assigner(\n",
    "            # pred_scores.detach().sigmoid() * 0.8 + dfl_conf.unsqueeze(-1) * 0.2,\n",
    "            pred_scores.detach().sigmoid(), # (batch, anchors, c)\n",
    "            (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype), # (b, h*w, 4)\n",
    "            anchor_points * stride_tensor, # (2, anchors)\n",
    "            gt_labels, # (B, counts.max, 1)\n",
    "            gt_bboxes, # (B, counts.max, 4)\n",
    "            mask_gt, # (B, counts.max, 1)\n",
    "        )\n",
    "\n",
    "        target_scores_sum = max(target_scores.sum(), 1)\n",
    "\n",
    "        # Cls loss\n",
    "        # loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\n",
    "        loss[1] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n",
    "\n",
    "        # Bbox loss\n",
    "        if fg_mask.sum():\n",
    "            target_bboxes /= stride_tensor\n",
    "            loss[0], loss[2] = self.bbox_loss(\n",
    "                pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask\n",
    "            )\n",
    "\n",
    "        loss[0] *= self.hyp[\"box\"]  # box gain\n",
    "        loss[1] *= self.hyp[\"cls\"]  # cls gain\n",
    "        loss[2] *= self.hyp[\"dfl\"]  # dfl gain\n",
    "\n",
    "        return loss * batch_size, loss.detach()  # loss(box, cls, dfl)\n",
    "        \n",
    "det_criterion = v8DetectionLoss(yolo.args, yolo.detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a38d8",
   "metadata": {},
   "source": [
    "**Segment loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy2xywh(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format where (x1, y1) is the\n",
    "    top-left corner and (x2, y2) is the bottom-right corner.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x, y, width, height) format.\n",
    "    \"\"\"\n",
    "    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
    "    y = torch.empty_like(x)  # faster than clone/copy\n",
    "    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n",
    "    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n",
    "    y[..., 2] = x[..., 2] - x[..., 0]  # width\n",
    "    y[..., 3] = x[..., 3] - x[..., 1]  # height\n",
    "    return y\n",
    "\n",
    "def crop_mask(masks, boxes):\n",
    "    \"\"\"\n",
    "    Crop masks to bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        masks (torch.Tensor): [n, h, w] tensor of masks.\n",
    "        boxes (torch.Tensor): [n, 4] tensor of bbox coordinates in relative point form.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Cropped masks.\n",
    "    \"\"\"\n",
    "    _, h, w = masks.shape\n",
    "    x1, y1, x2, y2 = torch.chunk(boxes[:, :, None], 4, 1)  # x1 shape(n,1,1)\n",
    "    r = torch.arange(w, device=masks.device, dtype=x1.dtype)[None, None, :]  # rows shape(1,1,w)\n",
    "    c = torch.arange(h, device=masks.device, dtype=x1.dtype)[None, :, None]  # cols shape(1,h,1)\n",
    "\n",
    "    return masks * ((r >= x1) * (r < x2) * (c >= y1) * (c < y2))\n",
    "\n",
    "\n",
    "class v8SegmentationLoss(v8DetectionLoss):\n",
    "    \"\"\"Criterion class for computing training losses for YOLOv8 segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, args, model_head):  # model must be de-paralleled\n",
    "        \"\"\"Initialize the v8SegmentationLoss class with model parameters and mask overlap setting.\"\"\"\n",
    "        super().__init__(args, model_head)\n",
    "        self.overlap = False\n",
    "\n",
    "    def __call__(self, preds, batch):\n",
    "        \"\"\"Calculate and return the combined loss for detection and segmentation.\"\"\"\n",
    "        loss = torch.zeros(4, device=self.device)  # box, seg, cls, dfl\n",
    "        feats, pred_masks, proto = preds if len(preds) == 3 else preds[1]\n",
    "        batch_size, _, mask_h, mask_w = proto.shape  # batch size, number of masks, mask height, mask width\n",
    "        pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(   \n",
    "            (self.reg_max * 4, self.nc), 1\n",
    "        )\n",
    " \n",
    "        # B, grids, ..\n",
    "        pred_scores = pred_scores.permute(0, 2, 1).contiguous() # (batch, c, anchors) -> (batch, anchors, c)\n",
    "        pred_distri = pred_distri.permute(0, 2, 1).contiguous() # (batch, 16*4, anchors) -> (batch, anchors, 16*4)\n",
    "        pred_masks = pred_masks.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        dtype = pred_scores.dtype\n",
    "        imgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\n",
    "        anchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n",
    "\n",
    "        # Targets\n",
    "        try:\n",
    "            batch_idx = batch[\"batch_idx\"].view(-1, 1)\n",
    "            targets = torch.cat((batch_idx, batch[\"cls\"].view(-1, 1), batch[\"bboxes\"]), 1)\n",
    "            targets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\n",
    "            gt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\n",
    "            mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0.0)\n",
    "        except RuntimeError as e:\n",
    "            raise TypeError(\n",
    "                \"ERROR ❌ segment dataset incorrectly formatted or not a segment dataset.\\n\"\n",
    "                \"This error can occur when incorrectly training a 'segment' model on a 'detect' dataset, \"\n",
    "                \"i.e. 'yolo train model=yolo11n-seg.pt data=coco8.yaml'.\\nVerify your dataset is a \"\n",
    "                \"correctly formatted 'segment' dataset using 'data=coco8-seg.yaml' \"\n",
    "                \"as an example.\\nSee https://docs.ultralytics.com/datasets/segment/ for help.\"\n",
    "            ) from e\n",
    "\n",
    "        # Pboxes\n",
    "        pred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n",
    "\n",
    "        _, target_bboxes, target_scores, fg_mask, target_gt_idx = self.assigner(\n",
    "            pred_scores.detach().sigmoid(),\n",
    "            (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\n",
    "            anchor_points * stride_tensor,\n",
    "            gt_labels,\n",
    "            gt_bboxes,\n",
    "            mask_gt,\n",
    "        )\n",
    "\n",
    "        target_scores_sum = max(target_scores.sum(), 1)\n",
    "\n",
    "        # Cls loss\n",
    "        # loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\n",
    "        loss[2] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n",
    "\n",
    "        if fg_mask.sum():\n",
    "            # Bbox loss\n",
    "            loss[0], loss[3] = self.bbox_loss(\n",
    "                pred_distri,\n",
    "                pred_bboxes,\n",
    "                anchor_points,\n",
    "                target_bboxes / stride_tensor,\n",
    "                target_scores,\n",
    "                target_scores_sum,\n",
    "                fg_mask,\n",
    "            )\n",
    "            # Masks loss\n",
    "            masks = batch[\"masks\"].to(self.device).float()\n",
    "            if tuple(masks.shape[-2:]) != (mask_h, mask_w):  # downsample\n",
    "                masks = F.interpolate(masks[None], (mask_h, mask_w), mode=\"nearest\")[0] # target mask를 proto의 h,w에 맞게 보간\n",
    "\n",
    "            loss[1] = self.calculate_segmentation_loss(\n",
    "                fg_mask, masks, target_gt_idx, target_bboxes, batch_idx, proto, pred_masks, imgsz, self.overlap\n",
    "            )\n",
    "\n",
    "        # WARNING: lines below prevent Multi-GPU DDP 'unused gradient' PyTorch errors, do not remove\n",
    "        else:\n",
    "            loss[1] += (proto * 0).sum() + (pred_masks * 0).sum()  # inf sums may lead to nan loss\n",
    "\n",
    "        loss[0] *= self.hyp[\"box\"]  # box gain\n",
    "        loss[1] *= self.hyp[\"box\"]  # seg gain\n",
    "        loss[2] *= self.hyp[\"cls\"]  # cls gain\n",
    "        loss[3] *= self.hyp[\"dfl\"]  # dfl gain\n",
    "\n",
    "        return loss * batch_size, loss.detach()  # loss(box, cls, dfl)\n",
    "\n",
    "    @staticmethod\n",
    "    def single_mask_loss(\n",
    "        gt_mask: torch.Tensor, pred: torch.Tensor, proto: torch.Tensor, xyxy: torch.Tensor, area: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the instance segmentation loss for a single image.\n",
    "\n",
    "        Args:\n",
    "            gt_mask (torch.Tensor): Ground truth mask of shape (n, H, W), where n is the number of objects.\n",
    "            pred (torch.Tensor): Predicted mask coefficients of shape (n, 32).\n",
    "            proto (torch.Tensor): Prototype masks of shape (32, H, W).\n",
    "            xyxy (torch.Tensor): Ground truth bounding boxes in xyxy format, normalized to [0, 1], of shape (n, 4).\n",
    "            area (torch.Tensor): Area of each ground truth bounding box of shape (n,).\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): The calculated mask loss for a single image.\n",
    "\n",
    "        Notes:\n",
    "            The function uses the equation pred_mask = torch.einsum('in,nhw->ihw', pred, proto) to produce the\n",
    "            predicted masks from the prototype masks and predicted mask coefficients.\n",
    "        \"\"\"\n",
    "        pred_mask = torch.einsum(\"in,nhw->ihw\", pred, proto)  # (n, 32) @ (32, 80, 80) -> (n, 80, 80)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred_mask, gt_mask, reduction=\"none\")\n",
    "        return (crop_mask(loss, xyxy).mean(dim=(1, 2)) / area).sum()\n",
    "\n",
    "    def calculate_segmentation_loss(\n",
    "        self,\n",
    "        fg_mask: torch.Tensor,\n",
    "        masks: torch.Tensor,\n",
    "        target_gt_idx: torch.Tensor,\n",
    "        target_bboxes: torch.Tensor,\n",
    "        batch_idx: torch.Tensor,\n",
    "        proto: torch.Tensor,\n",
    "        pred_masks: torch.Tensor,\n",
    "        imgsz: torch.Tensor,\n",
    "        overlap: bool,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the loss for instance segmentation.\n",
    "\n",
    "        Args:\n",
    "            fg_mask (torch.Tensor): A binary tensor of shape (BS, N_anchors) indicating which anchors are positive.\n",
    "            masks (torch.Tensor): Ground truth masks of shape (BS, H, W) if `overlap` is False, otherwise (BS, ?, H, W).\n",
    "            target_gt_idx (torch.Tensor): Indexes of ground truth objects for each anchor of shape (BS, N_anchors).\n",
    "            target_bboxes (torch.Tensor): Ground truth bounding boxes for each anchor of shape (BS, N_anchors, 4).\n",
    "            batch_idx (torch.Tensor): Batch indices of shape (N_labels_in_batch, 1).\n",
    "            proto (torch.Tensor): Prototype masks of shape (BS, 32, H, W).\n",
    "            pred_masks (torch.Tensor): Predicted masks for each anchor of shape (BS, N_anchors, 32).\n",
    "            imgsz (torch.Tensor): Size of the input image as a tensor of shape (2), i.e., (H, W).\n",
    "            overlap (bool): Whether the masks in `masks` tensor overlap.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): The calculated loss for instance segmentation.\n",
    "\n",
    "        Notes:\n",
    "            The batch loss can be computed for improved speed at higher memory usage.\n",
    "            For example, pred_mask can be computed as follows:\n",
    "                pred_mask = torch.einsum('in,nhw->ihw', pred, proto)  # (i, 32) @ (32, 160, 160) -> (i, 160, 160)\n",
    "        \"\"\"\n",
    "        _, _, mask_h, mask_w = proto.shape\n",
    "        loss = 0\n",
    "\n",
    "        # Normalize to 0-1\n",
    "        target_bboxes_normalized = target_bboxes / imgsz[[1, 0, 1, 0]]\n",
    "\n",
    "        # Areas of target bboxes\n",
    "        marea = xyxy2xywh(target_bboxes_normalized)[..., 2:].prod(2)\n",
    "\n",
    "        # Normalize to mask size\n",
    "        mxyxy = target_bboxes_normalized * torch.tensor([mask_w, mask_h, mask_w, mask_h], device=proto.device)\n",
    "\n",
    "        for i, single_i in enumerate(zip(fg_mask, target_gt_idx, pred_masks, proto, mxyxy, marea, masks)):\n",
    "            fg_mask_i, target_gt_idx_i, pred_masks_i, proto_i, mxyxy_i, marea_i, masks_i = single_i\n",
    "            if fg_mask_i.any():\n",
    "                mask_idx = target_gt_idx_i[fg_mask_i]\n",
    "                if overlap:\n",
    "                    gt_mask = masks_i == (mask_idx + 1).view(-1, 1, 1)\n",
    "                    gt_mask = gt_mask.float()\n",
    "                else:\n",
    "                    gt_mask = masks[batch_idx.view(-1) == i][mask_idx]\n",
    "\n",
    "                loss += self.single_mask_loss(\n",
    "                    gt_mask, pred_masks_i[fg_mask_i], proto_i, mxyxy_i[fg_mask_i], marea_i[fg_mask_i]\n",
    "                )\n",
    "\n",
    "            # WARNING: lines below prevents Multi-GPU DDP 'unused gradient' PyTorch errors, do not remove\n",
    "            else:\n",
    "                loss += (proto * 0).sum() + (pred_masks * 0).sum()  # inf sums may lead to nan loss\n",
    "\n",
    "        return loss / fg_mask.sum()\n",
    "        \n",
    "seg_criterion = v8SegmentationLoss(yolo.args, yolo.segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5960ed5c",
   "metadata": {},
   "source": [
    "**ResNetMLP loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46241574",
   "metadata": {},
   "outputs": [],
   "source": [
    "resmlp_criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8466c66",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingAndCheckpoint:\n",
    "    def __init__(self, patience=10, model_name=\"\", improvement_path='model.pt', no_improvement_path='temp_model.pt'):\n",
    "        self.model_name = model_name\n",
    "        self.patience = patience            # 몇 번 참을지\n",
    "        self.improvement_path = improvement_path\n",
    "        self.no_improvement_path = no_improvement_path\n",
    "\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_train_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, val_loss, model, epoch, loss_text):\n",
    "        if self.early_stop == False:\n",
    "            \n",
    "            # 조건 1: 성능 향상 모델 저장 조건 확인\n",
    "            val_loss_sum = sum(val_loss)\n",
    "            train_loss_sum = sum(train_loss)\n",
    "            if (self.best_val_loss > val_loss_sum) and (self.best_train_loss > train_loss_sum):\n",
    "                torch.save(model.state_dict(), self.improvement_path)\n",
    "                wandb.save(self.improvement_path)\n",
    "                \n",
    "                #print(f\"Checkpoint saved! train_loss={train_loss:.5f}, val_loss={val_loss:.5f}\")\n",
    "                print(f\"{self.model_name}. Checkpoint saved! Epoch {epoch+1}.\",\"\\n\", \n",
    "                      \"train_loss => \",[f\"{t}:{x:.5f}\" for t, x in zip(loss_text, train_loss)],\"\\n\",\n",
    "                      \"val_loss => \",[f\"{t}:{x:.5f}\" for t, x in zip(loss_text, val_loss)]\n",
    "                     )\n",
    "                self.best_val_loss = val_loss_sum\n",
    "                self.best_train_loss = train_loss_sum\n",
    "                self.counter = 0\n",
    "    \n",
    "            # 조건 2: 조기 종료 조건\n",
    "            elif (val_loss_sum > self.best_val_loss):\n",
    "                \n",
    "                torch.save(model.state_dict(), self.no_improvement_path)\n",
    "                wandb.save(self.no_improvement_path)\n",
    "                self.counter += 1\n",
    "                \n",
    "                #print(f\"No improvement. train_loss={train_loss:.5f}, val_loss={val_loss:.5f}. Patience counter: {self.counter}/{self.patience}\")\n",
    "                print(f\"{self.model_name}. No improvement. Epoch {epoch+1}.\",\"\\n\", \n",
    "                  \"train_loss => \",[f\"{t}:{x:.5f}\" for t, x in zip(loss_text, train_loss)],\"\\n\",\n",
    "                  \"val_loss => \",[f\"{t}:{x:.5f}\" for t, x in zip(loss_text, val_loss)],\n",
    "                    f\"Patience counter: {self.counter}/{self.patience}\"\n",
    "                 )\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "        else:\n",
    "            print(f\"{self.model_name}. early stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084538f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect & segment \n",
    "def train_model(yolo, epochs, \n",
    "                det_criterion, seg_criterion, \n",
    "                optimizer,\n",
    "                train_loader, val_loader, \n",
    "                early_stopper,\n",
    "                scheduler,\n",
    "                make_bacteria\n",
    "               ):\n",
    "    \n",
    "    start = time.time() #학습이 얼마나 걸리는 지 확인 (시작)\n",
    "    \n",
    "    det_loss_text = [\"box loss\", \"class loss\", \"dfl loss\"]\n",
    "    #seg_loss_text = [\"box loss\", \"seg loss\", \"class loss\", \"dfl loss\"]\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        yolo.train() \n",
    "        det_train_total_loss = torch.tensor([0., 0., 0.])\n",
    "        seg_train_total_loss = torch.tensor([0., 0., 0., 0.])\n",
    "\n",
    "        for data in  tqdm(train_loader): \n",
    "            images, labels = data[0].to(device), data[1]\n",
    "     \n",
    "            #---------------------------\n",
    "            # Detect training\n",
    "            #---------------------------\n",
    "            YOLOv8.mode = \"detect\"\n",
    "            outputs = yolo(images)\n",
    "            \n",
    "            det_loss = det_criterion(outputs, labels) \n",
    "            det_loss_rec = det_loss[1].to('cpu') \n",
    "            \n",
    "            det_train_total_loss += det_loss_rec\n",
    "\n",
    "            det_loss[0].sum().backward() \n",
    "            optimizer.step() \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #---------------------------\n",
    "            # Segment training\n",
    "            #---------------------------\n",
    "            YOLOv8.mode = \"segment\"\n",
    "            images, labels = make_bacteria.batch_generate(batch_size = 4)\n",
    "            images = images.to(device)\n",
    "            \n",
    "            outputs = yolo(images)\n",
    "            \n",
    "            seg_loss = seg_criterion(outputs, labels) \n",
    "            seg_loss_rec = seg_loss[1].to('cpu') \n",
    "            \n",
    "            seg_train_total_loss += seg_loss_rec\n",
    "\n",
    "            (seg_loss[0] * 0.1).sum().backward() # segment는 0.1 가중치를 주어서 조금만 학습\n",
    "            optimizer.step() \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_lr = yolo_optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            wandb.log({\"learning rate\" : current_lr, \n",
    "                       \"Detect train box loss\" : det_loss_rec[0], \"Detect train class loss\": det_loss_rec[1], \"Detect train dfl loss\" : det_loss_rec[2], \n",
    "                       \"Segment train box loss\" : seg_loss_rec[0], \"Segment train segment loss\": seg_loss_rec[1], \"Segment train class loss\": seg_loss_rec[2], \"Segment train dfl loss\" : seg_loss_rec[3]\n",
    "                       })\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            yolo.eval() \n",
    "            det_val_total_loss = torch.tensor([0., 0., 0.])\n",
    "            seg_val_total_loss = torch.tensor([0., 0., 0., 0.])\n",
    "\n",
    "            for data in tqdm(val_loader): \n",
    "                images, labels = data[0].to(device), data[1]\n",
    "                    \n",
    "                #---------------------------\n",
    "                # Detect evaluation\n",
    "                #---------------------------\n",
    "                YOLOv8.mode = \"detect\"\n",
    "                outputs = yolo(images) \n",
    "                det_loss = det_criterion(outputs, labels)\n",
    "                det_val_total_loss += det_loss[1].to('cpu')\n",
    "                \n",
    "                #---------------------------\n",
    "                # Segment evaluation\n",
    "                #---------------------------\n",
    "                YOLOv8.mode = \"segment\"\n",
    "                images, labels = make_bacteria.batch_generate(batch_size = 4)\n",
    "                images = images.to(device)\n",
    "                outputs = yolo(images) \n",
    "                seg_loss = seg_criterion(outputs, labels)\n",
    "                seg_val_total_loss += seg_loss[1].to('cpu')\n",
    "\n",
    "                \n",
    "        det_train_avg_loss = (det_train_total_loss / len(train_loader)).tolist()\n",
    "        det_val_avg_loss = (det_val_total_loss / len(val_loader)).tolist()\n",
    "        \n",
    "        seg_train_avg_loss = (seg_train_total_loss / len(train_loader)).tolist()\n",
    "        seg_val_avg_loss = (seg_val_total_loss / len(val_loader)).tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "        wandb.log({\"epoch\": epoch, \n",
    "                   \"Detect train avg box loss\": det_train_avg_loss[0], \"Detect train avg class loss\": det_train_avg_loss[1], \"Detect train avg dfl loss\": det_train_avg_loss[2],\n",
    "                   \"Detect validation avg box loss\": det_val_avg_loss[0],\"Detect validation avg class loss\": det_val_avg_loss[1], \"Detect validation avg dfl loss\": det_val_avg_loss[2],\n",
    "                   \n",
    "                   \"Segment train avg box loss\": seg_train_avg_loss[0], \"Segment train avg segment loss\": seg_train_avg_loss[1], \"Segment train avg class loss\": seg_train_avg_loss[2], \"Segment train avg dfl loss\": seg_train_avg_loss[3],\n",
    "                   \"Segment validation avg box loss\": seg_val_avg_loss[0],\"Segment validation avg segment loss\": seg_train_avg_loss[1],\"Segment validation avg class loss\": seg_val_avg_loss[2], \"Segment validation avg dfl loss\": seg_val_avg_loss[3]\n",
    "                  })\n",
    "                 \n",
    "        early_stopper(det_train_avg_loss, det_val_avg_loss, yolo, epoch, det_loss_text)\n",
    "            \n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "          \n",
    "    end = time.time() #학습이 얼마나 걸리는 지 확인 (끝)\n",
    "    sec = end-start\n",
    "    \n",
    "    print(\"Training Done.\")\n",
    "    print(f\"Elapsed Time : {int(sec)//60} minutes, {sec%60:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "epochs = 16\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=\"simple-rule-project\",\n",
    "    project=\"BYU - Locating Bacterial Flagellar Motors 2025\",\n",
    "     id=\"bj017d94\",  \n",
    "    resume=\"must\",\n",
    "    config={\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\" : batch_size,\n",
    "        \"setting\" : \"detct & segment multi head\"\n",
    "    },\n",
    ")\n",
    "\n",
    "yolo_optimizer = optim.AdamW(yolo.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "yolo_scheduler = OneCycleLR(\n",
    "    yolo_optimizer,\n",
    "    max_lr=0.0048,                # 학습률 최고점\n",
    "    steps_per_epoch=len(train_loader),  # 한 에폭 안의 스텝 수\n",
    "    epochs=epochs,                 # 전체 epoch 수\n",
    "    pct_start=0.000001,             # 얼마나 빨리 최고점에 도달할지 (10% 지점에서 최고)\n",
    "    anneal_strategy='cos',     # 감소 방식: 'cos' 또는 'linear'\n",
    "    final_div_factor=1e4       # 마지막 lr은 max_lr / final_div_factor\n",
    ")\n",
    "\n",
    "yolo_early_stopper = EarlyStoppingAndCheckpoint(\n",
    "    patience=5,\n",
    "    model_name=\"Yolo\",\n",
    "    improvement_path='best_multi_head_trained_yolo.pt', \n",
    "    no_improvement_path='temp_multi_head_trained_yolo.pt'\n",
    ")\n",
    "\n",
    "make_bacteria = Make_bacteria_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c230a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(yolo, epochs, \n",
    "            det_criterion, seg_criterion, \n",
    "            yolo_optimizer,\n",
    "            train_loader, val_loader, \n",
    "            yolo_early_stopper,\n",
    "            yolo_scheduler,\n",
    "            make_bacteria\n",
    "           )\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36ed91",
   "metadata": {},
   "source": [
    "### ResNetMLP Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, \n",
    "                criterion, \n",
    "                optimizer, \n",
    "                train_loader, val_loader, \n",
    "                early_stopper, \n",
    "                scheduler\n",
    "               ):\n",
    "    \n",
    "    start = time.time() #학습이 얼마나 걸리는 지 확인 (시작)\n",
    "    \n",
    "    loss_text = [\"Vit loss\"]\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train() \n",
    "        train_total_loss = torch.tensor([0.])\n",
    "        \n",
    "        for data in  tqdm(train_loader): \n",
    "            data = make_z_label(data) \n",
    "            if data[0] != None:\n",
    "                cropped_images, labels = data[0].to(device), data[1].to(device)\n",
    "                \n",
    "                outputs = model(cropped_images)\n",
    "                \n",
    "                loss = criterion(outputs, labels) \n",
    "                loss_rec = loss.item()\n",
    "    \n",
    "                loss.backward() \n",
    "                optimizer.step() \n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                \n",
    "                train_total_loss += loss_rec\n",
    "            \n",
    "                wandb.log({\"learning rate\" : current_lr, \n",
    "                           \"train loss\": loss_rec})\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            model.eval() \n",
    "            val_total_loss = torch.tensor([0.])\n",
    "            for data in tqdm(val_loader):       \n",
    "                data = make_z_label(data)\n",
    "                if data[0] != None:\n",
    "                    cropped_images, labels = data[0].to(device), data[1].to(device)\n",
    "                    \n",
    "                    outputs = model(cropped_images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_total_loss += loss.item()\n",
    "        \n",
    "        train_avg_loss = (train_total_loss / len(train_loader)).tolist()\n",
    "        val_avg_loss = (val_total_loss / len(val_loader)).tolist()\n",
    "\n",
    "        wandb.log({\"epoch\": epoch, \n",
    "                   \"train avg loss\": train_avg_loss[0], \"validation avg loss\": val_avg_loss[0]\n",
    "                  })\n",
    "                 \n",
    "        early_stopper(train_avg_loss, val_avg_loss, model, epoch, loss_text)\n",
    "            \n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "          \n",
    "    end = time.time() #학습이 얼마나 걸리는 지 확인 (끝)\n",
    "    sec = end-start\n",
    "    \n",
    "    print(\"Training Done.\")\n",
    "    print(f\"Elapsed Time : {int(sec)//60} minutes, {sec%60:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "epochs = 50\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=\"simple-rule-project\",\n",
    "    project=\"BYU - Locating Bacterial Flagellar Motors 2025\",\n",
    "    config={\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\" : batch_size,\n",
    "        \"backbone\": \"resnet18\", \n",
    "        \"mlp\": \"4 layers\", \n",
    "        \"output\": \"11 class\"\n",
    "    })\n",
    "\n",
    "resmlp_optimizer = optim.AdamW(resnet_mlp.parameters(), lr=0.005, weight_decay=0.01)\n",
    "\n",
    "resmlp_scheduler = OneCycleLR(\n",
    "    resmlp_optimizer,\n",
    "    max_lr=0.005,                # 학습률 최고점\n",
    "    steps_per_epoch=len(train_loader),  # 한 에폭 안의 스텝 수\n",
    "    epochs=epochs,                 # 전체 epoch 수\n",
    "    pct_start=0.1,             # 얼마나 빨리 최고점에 도달할지 (1% 지점에서 최고)\n",
    "    anneal_strategy='cos',     # 감소 방식: 'cos' 또는 'linear'\n",
    "    final_div_factor=1e4       # 마지막 lr은 max_lr / final_div_factor\n",
    ")\n",
    "\n",
    "resmlp_early_stopper = EarlyStoppingAndCheckpoint(\n",
    "    patience=5,\n",
    "    model_name=\"ResNet_MLP\",\n",
    "    improvement_path='best_resnet_mlp.pt', \n",
    "    no_improvement_path='temp_resnet_mlp.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(resnet_mlp, epochs, \n",
    "            resmlp_criterion, \n",
    "            resmlp_optimizer, \n",
    "            train_loader, val_loader, \n",
    "            resmlp_early_stopper, \n",
    "            resmlp_scheduler\n",
    "           )\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffbca3f",
   "metadata": {},
   "source": [
    "## 6. 제출\n",
    "\n",
    "![image](/assets/images/BYU images/submission1.png){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "public 점수가 **0.25~0.3점**이었습니다(왼쪽이 대회 종료 후 공개된 Private 점수, 오른쪽이 public 점수). **baseline 점수에 크게 못미치는 점수**였습니다. \n",
    "\n",
    "추후 detection head로만 학습했을 때와 segmentation head를 함께 muti-head로 학습한 결과를 비교했을 때, **detection head로만 학습한 모델의 성능이 더 좋았습니다.** 그러나 점수의 향상이 미미했습니다.\n",
    "\n",
    "실험 결과, **Multi-head 방식이 도움이 되지 않았습니다.** \n",
    "\n",
    "## 7. Yolo11로 재실험\n",
    "\n",
    "마지막으로 지금까지 했던 실험에 기반하여 새롭게 모델을 학습시켜 보기로 했습니다.\\\n",
    "**Multi-head**로 학습하는 것이 도움이 안된다는 것을 알았으니 **Detection**만 하기로 했습니다. 그리고 yolo8 모델 대신, 라이브러리 구현체에서 제공되는 가장 최신 모델인 **yolo11**을 사용하기로 했습니다. yolo11는 **3채널** 이상 받을 수 있도록 개조하지 않고 **3채널** 입력 그대로 사용하기로 했습니다. **8장 간격**으로 **3장**을 채널 방향으로 쌓아 하나의 이미지로 만들었습니다.\\\n",
    "라이브러리 구현체를 그대로 사용 했을 때 내부 코드로 구현되어 있는 다양한 **augmentation**들을 쉽게 활용할 수 있다는 장점이 있습니다.\\\n",
    "yolo11의 모델 중 가장 사이즈가 큰 **yolo11x**를 사용했습니다.\n",
    "\n",
    "아래가 **yolo11** 모델을 학습시키는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd03e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"/kaggle/input/yolo11x/pytorch/default/4/17epochs_best.pt\")\n",
    "\n",
    "yolo_weights_dir = \"/kaggle/working/yolo_weights\"\n",
    "os.makedirs(yolo_weights_dir, exist_ok=True)\n",
    "yaml_path = \"/kaggle/input/d/jobidan/byu-3channels-train-dataset/byu-3channels-train-dataset.yaml\"\n",
    "results = model.train(data=yaml_path, \n",
    "                      epochs=50, \n",
    "                      imgsz=640, \n",
    "                      device=[0, 1],\n",
    "                      patience=6,\n",
    "                      batch=16,\n",
    "                      single_cls=True,\n",
    "                      project=yolo_weights_dir,\n",
    "                      name='motor_detector',\n",
    "                      exist_ok=True,\n",
    "                      lr0=0.003, # lr0 * 0.01까지 줄어듬\n",
    "                      warmup_epochs=0.0001, # 스케줄러 warmup epoch \n",
    "                      cls = 0.2,\n",
    "                      val = True,\n",
    "                      save_period=2,\n",
    "                      \n",
    "                      hsv_h = 0,\n",
    "                      hsv_s = 0,\n",
    "                      degrees = 0.1\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aacf56",
   "metadata": {},
   "source": [
    "![image](/assets/images/BYU images/submission2.png){: width=\"100%\" height=\"100%\"}\n",
    "\n",
    "제출 결과 점수는 **0.4**점대로 기존 **0.2**점대에서는 올랐지만 여전히 **baseline**에도 못 미치는 점수입니다.\\\n",
    "detection의 특성상 bbox 외의 영역이 전부 negative class에 속하기 때문에 라벨(bbox)이 존재하는 이미지 자체도 이미 **class unbalance**가 있습니다. 그런데 라벨(bbox)이 아예 없는 데이터까지 많이 학습하면 **class unbalance**가 심해집니다. baseline에서 정답 z 주변의 아주 적은 이미지만을 사용해서 학습을 했지만 그럼에도 저의 모델보다 성능이 잘 나온 이유가 **class unbalance** 때문이라고 예상합니다.\n",
    "\n",
    "# 배운 점\n",
    "\n",
    "이번 대회를 통해 **소통**의 중요성을 다시 한 번 깨달았습니다.\\\n",
    "캐글 대회에 참가한다면 개발자들이 소통하는 **Discussion**을 계속 살펴보는 것이 중요합니다. 여기에 놓치기 쉽거나 중요한 정보가 많습니다. 예를 들어, 제출할 때 데이터 타입을 integer 대신 floating point로 해야 점수가 잘 나오며, 라벨 누락 등의 문제가 있다는 이야기가 있었습니다. 이를 잘 처리하는 것도 주된 문제였습니다.\n",
    "\n",
    "모델의 **일반화**도 매우 중요하는 것을 느꼈습니다.\\\n",
    "**public 점수**로 순위권 안에 들던 팀들이 대회가 끝나고 **비공개 테스트 데이터**로 **평가**되었을 때, 1등을 제외한 모든 팀이 순위권 밖으로 밀려났습니다. 1등한 참가자의 후기를 보았을 때 일반화에 노력을 많이 한 것이 보였습니다. 기업이 AI 모델을 개발할 때도 일반화가 중요할 것이라고 느꼈습니다. \n",
    "\n",
    "실패를 했을 때 **실패를 통해 배우는 것이 중요**하다고 느꼈습니다.\\\n",
    "대회에서 수상하지 못하고 \"시간 낭비였나?\"라는 생각을 했을 때, 실패를 하지 못했으면 발전하지 못했을 거란 생각을 했습니다. 1등한 참가자가 작성한 솔루션을 통해 그의 통찰과 전략을 배우기도 했습니다. 제가 시행착오를 겪으며 문제를 깊게 이해했기에 다른 참가자의 솔루션을 더 깊이있게 배울 수 있었다고 생각합니다.\n",
    "\n",
    "1등 참가자의 솔루션을 공유하며 글을 마무리하겠습니다.\n",
    "\n",
    "[1st Place Solution - 3D U-Net + Quantile Thresholding ](https://www.kaggle.com/competitions/byu-locating-bacterial-flagellar-motors-2025/writeups/bartley-1st-place-3d-u-net-quantile-thresholding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a1a45",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
